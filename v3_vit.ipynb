{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Added Params (for training and testing)\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512 \n",
    "EPOCHS = 40 ##why this high number? usually for transformers you do 1,2,3. \n",
    "\n",
    "##\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "PATCH_SIZE = 4 #we chose 4-> pixel length of 1 dimension\n",
    "IMAGE_SIZE = 56 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_HEADS = 8 #Within the transformer encoder there are attention heads- we choose 8 of them.                           \n",
    "DROPOUT = 0.001 \n",
    "HIDDEN_DIM = 768 #hidden dimentsion of MLP head for classification \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "\n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "NUM_ENCODER = 4 #stack encoders on top of each other (architecture just shows one)\n",
    "\n",
    "\n",
    "##This is the input size to the patch embedding layer (aka flattening image into sequence of patches )\n",
    "EMBED_DIM = 64 # 16 -> basically the number of patches\n",
    "\n",
    "## Paper defines the below as: N =HW / P^2\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cude\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERWRITE PARAMS TO TRAIN ON MY COMPUTER\n",
    "BATCH_SIZE = 512 #change to 256 if memory cant handle 512\n",
    "EPOCHS = 6\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 128\n",
    "NUM_ENCODER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 197, 64])\n"
     ]
    }
   ],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        #function that divides images into patches\n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) #equivalent to nn.Flatten(start_dim=2, end_dim=-1) -> not a learnable layer (converts patched into sequence of vectors)\n",
    "        \n",
    "            #OUTPUT SHAPE: (batch_size, embedding_dim, num_patches) AKA the full sequence of patches\n",
    "            \n",
    "        \n",
    "        #---- CLS Token ---- \n",
    "     \n",
    "        #here we define the [CLS] token. nn.Parameter is a learnable tensor (its a single parameter not a full layer)\n",
    "        # Create a random tensor of shape (1, in_channels, embedding_dim), wrap it as a learnable parameter, and assign it as the CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embedding_dim)), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        #---- Positional Embedding ---- \n",
    "        \n",
    "        \n",
    "        #positional embedding is a learnable parameter \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches+1,embedding_dim)), requires_grad=True) #we add 1 to num_patches because we have the [CLS] token\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    #after patching and flattening we have a tesnor of shape (batch_size, embedding_dim, num_patches) e.g., (32, 16, 49)\n",
    "    # x = x.permute(0, 2, 1) rearranges to (batch_size, num_patches, embedding_dim) e.g., (32, 49, 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        #here we expand the cls token so its not just the shape for 1 sample but for a batch of images\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) #expand the cls token to the batch size. x.shape[0] is the batch size. -1, -1 tells expand function to keep original dimensions. \n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        \n",
    "        #1 axis for batches, 1 axis for sequence of patches, 1 axis for embedding dimension \n",
    "        x = torch.cat([cls_token, x], dim=1) #so we want to add the CLS token to the left of the patches\n",
    "        \n",
    "        #then we need to add the position tokens to each patch \n",
    "        x = self.position_embedding + x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#always test model after you define it    \n",
    "model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B, S, D)\n",
    "        q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = self.drop(attn.softmax(dim=-1))            # (B,S,S)\n",
    "        return attn @ v                                   # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden dim not divisible by heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B,S,D)\n",
    "        # list of (B,S,d_h) → cat on last dim → (B,S,D)\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.drop(self.out_proj(x))                # (B,S,D)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n",
    "\n",
    "# === 6. ViT classifier head (uses Encoder) ====================================\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_patches: int = NUM_PATCHES,\n",
    "                 num_classes: int = NUM_CLASSES,\n",
    "                 patch_size: int = PATCH_SIZE,\n",
    "                 embed_dim: int = EMBED_DIM,\n",
    "                 depth: int = NUM_ENCODER,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 in_channels: int = IN_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.embed = PatchEmbedding(embed_dim, patch_size,\n",
    "                                    num_patches, dropout, in_channels)\n",
    "        self.encoder = Encoder(depth, embed_dim, num_heads, dropout)\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B,C,H,W)\n",
    "        x = self.embed(x)                                 # (B,S,D)\n",
    "        x = self.encoder(x)                               # (B,S,D)\n",
    "        cls = x[:, 0]                                     # (B,D)\n",
    "        return self.cls_head(cls)                         # (B,num_classes)\n",
    "\n",
    "\n",
    "model = ViT(NUM_PATCHES, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODER, NUM_HEADS, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 56, 56) #dummy image\n",
    "print(model(x).shape) #expect [512, 10] -> batch of 512 (512 images) for 10 classes -> returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Synthetic 2×2–grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56×56   PatchSize: 4  → 14×14 = 196 patch tokens\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "SEQ_MAX_LEN  = 6                    #  <start> + up-to-4 digits + <finish>\n",
    "\n",
    "GRID_SIZE    = 2                    # 2×2 cells\n",
    "CELL_PIX     = 28\n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX # 56\n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # uint8 → 0-1\n",
    "    transforms.Normalize([0.5], [0.5])      # centre to −1..1\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels,\n",
    "                 epoch_size=60_000, rng=None):\n",
    "        self.base_images = base_images          # (N,28,28) uint8\n",
    "        self.base_labels = base_labels          # (N,)\n",
    "        self.epoch_size  = epoch_size\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "\n",
    "        # indices per digit for fast balanced sampling\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "        # 15 non-empty cell-occupancy patterns (bitmask 1..15)\n",
    "        patterns = np.arange(1, 16)\n",
    "        reps = math.ceil(epoch_size / len(patterns))\n",
    "        self.pattern_pool = self.rng.permutation(\n",
    "            np.tile(patterns, reps)[:epoch_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _cells_from_mask(mask: int):\n",
    "        return [i for i in range(4) if (mask >> i) & 1]   # TL,TR,BL,BR\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask     = int(self.pattern_pool[idx])\n",
    "        cell_ids = self._cells_from_mask(mask)\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        seq    = [VOCAB['<start>']]\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))\n",
    "            img_idx = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "\n",
    "            row, col = divmod(cell, GRID_SIZE)\n",
    "            top, left = row*CELL_PIX, col*CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "\n",
    "            seq.append(VOCAB[str(d)])\n",
    "\n",
    "        seq.append(VOCAB['<finish>'])\n",
    "        length = len(seq)\n",
    "        seq += [PAD_IDX] * (SEQ_MAX_LEN - length)\n",
    "\n",
    "        return {\n",
    "            'image'  : grid_transforms(canvas),          # (1,56,56) float\n",
    "            'target' : torch.tensor(seq, dtype=torch.long),  # (6,)\n",
    "            'length' : length\n",
    "        }\n",
    "\n",
    "# ─── build train / val / test loaders ───────────────────────────────────────\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),   # we reuse MNIST test set\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "train_dataloader = DataLoader(train_grid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader   = DataLoader(val_grid,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader  = DataLoader(test_grid,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}\n",
    "\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt  = ' '.join([str(VOCAB_INV[t]) for t in target if t != PAD_IDX])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we get into training loop \n",
    "\n",
    "#define loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=ADAM_WEIGHT_DECAY, betas=ADAM_BETAS)\n",
    "\n",
    "#lets estimate the time it will take to train the model \n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "#we add 2 more parameters to the tqdm function: position and leave. \n",
    "for epoch in tqdm(range(EPOCHS), position =0, leave=True):\n",
    "    model.train() #take the model into train mode \n",
    "    \n",
    "    #we need to store train labels and predictions for a given epoch so we can calculate the loss and accuracy for that epoch so we initialise empty lists \n",
    "    train_labels = [] \n",
    "    train_preds = []\n",
    "    train_running_loss = 0.0 #store runnign losss which will start at 0\n",
    "     \n",
    "    #iterate through data loader \n",
    "    for idx, img_label in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        #remember we return 'image' from dataset class and its a dataframe which is accessible like a dictionary\n",
    "        img = img_label['image'].float().to(device)\n",
    "        label = img_label['label'].type(torch.uint8).to(device) # remember MNIST labels are always integers between 0–9 (digits) so we store as int\n",
    "        y_pred = model(img)\n",
    "        \n",
    "        #recall the output shape is (batch_size, num_classes) aka ([512, 10]), which is a probability distribution over the 10 classes -> so we need to get the index of the highest probability\n",
    "        \n",
    "        \n",
    "        #to predict the label: \n",
    "        y_pred_label = torch.argmax(y_pred, dim=1) #take the column with the highest probability and select it -> dim 1 instead of 0 because we look across the rows of [512, 10] which is the classes\n",
    "        \n",
    "        #add to train labels. store to cpu when we can to avoid using up GPU memory \n",
    "        train_labels.extend(label.cpu().detach())\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "        \n",
    "        #3 lines below ensure that training is happening \n",
    "        \n",
    "        loss = criterion(y_pred, label) #calculate the loss -> returns a scalar tensor\n",
    "        optimizer.zero_grad() #zero the parameter gradients\n",
    "        loss.backward() #backpropagate the loss\n",
    "        optimizer.step() #update the model parameters\n",
    "        \n",
    "        #update the loss \n",
    "        train_running_loss += loss.item() #stores the loss for all batches in the epoch\n",
    "    \n",
    "    #now we update the train loss for the whole epoch     \n",
    "    train_loss = train_running_loss / len(train_dataloader) #stores the loss for 1 epoch -> average the loss over the number of batches in the epoch. so we have average loss per batch for 1 epoch. len(train_dataloader) = number of batches in the epoch\n",
    "    \n",
    "    \n",
    "    #calculate train accuracy (num of correct predictions / total num of predictions)\n",
    "    train_acc = sum(x == y for x, y in zip(train_labels, train_preds)) / len(train_labels)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    #take model into evaluation mode \n",
    "    model.eval()\n",
    "    val_labels = [] #store validation labels for a given epoch \n",
    "    val_preds = [] #store validation predictions for a given epoch \n",
    "    val_running_loss = 0.0 #store running loss for a given epoch \n",
    "    \n",
    "    #no need to track gradients in validation loop aka ensures no learning is happening in validation loop -> since we just want validation score from trained model \n",
    "    with torch.no_grad():\n",
    "        for idx, img_label in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
    "            img = img_label['image'].float().to(device)\n",
    "            label = img_label['label'].type(torch.uint8).to(device)\n",
    "            y_pred = model(img)\n",
    "            \n",
    "            y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "            \n",
    "            val_labels.extend(label.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "            \n",
    "            loss = criterion(y_pred, label)\n",
    "            val_running_loss += loss.item()\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = sum(x == y for x, y in zip(val_labels, val_preds)) / len(val_labels)\n",
    "\n",
    "            \n",
    "     #print as train happens \n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train loss EPOCH {epoch+1}: {train_loss:.4f}\")       \n",
    "    print(f\"Val loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(f\"Train acc EPOCH {epoch+1}: {train_acc:.4f}\")\n",
    "    print(f\"Val acc EPOCH {epoch+1}: {val_acc:.4f}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "stop = timeit.default_timer() \n",
    "print(f\"Time taken: {stop - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer loop: \n",
    "- we loop over epochs\n",
    "\n",
    "Inner loop: \n",
    "- we loop over each batch from the dataloader \n",
    "\n",
    "\n",
    "You will see: e.g. 46/106 -> 106 is the number of batches. number of batches = dataset-size/batch-size = (54K) /512 =106 (round up) for train, 6k/512 = 12 (round up) for test. We always round up, last batch may be smaller but we still train on those samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue with prediction on test set \n",
    "\n",
    "labels_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n",
    "        img = sample['image'].float().to(device)\n",
    "\n",
    "        y_pred = model(img)\n",
    "        y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        labels_pred.extend([int(i) for i in y_pred_label.cpu().detach()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot test data and see our predictions \n",
    "plt.figure()\n",
    "f, axarr = plt.subplots(2, 3)\n",
    "counter = 0\n",
    "\n",
    "for i in range(2): #iterate through rows in plot \n",
    "    for j in range(3): #iterate through columns in plot \n",
    "        axarr[i, j].imshow(img[counter].squeeze(), cmap='gray')\n",
    "        axarr[i, j].set_title(f\"Predicted: {labels_pred[counter]}\")\n",
    "        counter += 1\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
