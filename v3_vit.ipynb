{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as TF   #  <-- add this line\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PATCHES = 49\n"
     ]
    }
   ],
   "source": [
    "## Added Params (for training and testing)\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512 \n",
    "EPOCHS = 40 ##why this high number? usually for transformers you do 1,2,3. \n",
    "\n",
    "##\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "PATCH_SIZE = 4 \n",
    "IMAGE_SIZE = 28 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_HEADS = 8 #Within the transformer encoder there are attention heads- we choose 8 of them.                           \n",
    "DROPOUT = 0.001 \n",
    "HIDDEN_DIM = 768 #hidden dimentsion of MLP head for classification \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "\n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "NUM_ENCODER = 4 #stack encoders on top of each other (architecture just shows one)\n",
    "\n",
    "\n",
    "##This is the input size to the patch embedding layer (aka flattening image into sequence of patches )\n",
    "EMBED_DIM = 64 # 16 -> basically the number of patches\n",
    "\n",
    "## Paper defines the below as: N =HW / P^2\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"NUM_PATCHES =\", NUM_PATCHES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERWRITE PARAMS TO TRAIN ON MY COMPUTER\n",
    "BATCH_SIZE = 256 #change to 256 if memory cant handle 512\n",
    "EPOCHS = 10\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 128\n",
    "NUM_ENCODER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "#function that divides images into patches\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) \n",
    "       \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches,embedding_dim)), requires_grad=True)  #Positional Embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        x = self.position_embedding + x #add posi\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# #always test model after you define it    \n",
    "# model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "# x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "# print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask=None):                                 # x: (B, S, D)\n",
    "        # q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        q = self.q_proj(q_in) # (B, Seq_q, d_h)\n",
    "        k = self.k_proj(k_in) # (B, Seq_k, d_h)\n",
    "        v = self.v_proj(v_in) # (B, Seq_v, d_h)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "            \n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.drop(attn)            # (B,S,S)\n",
    "        context = attn @ v \n",
    "        return context                                 # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):          \n",
    "        \"\"\"\n",
    "        q : (B, Seq_q, D)\n",
    "        k : (B, Seq_k, D)   defaults to q if None\n",
    "        v : (B, Seq_k, D)   defaults to k if None\n",
    "        mask : (1, 1, Seq_q, Seq_k) or None\n",
    "        \"\"\"\n",
    "        k = q if k is None else k\n",
    "        v = q if v is None else v\n",
    "\n",
    "        # run every head, collect their (B, Seq_q, d_h) outputs\n",
    "        head_outputs = [\n",
    "            head(q, k, v, mask) for head in self.heads\n",
    "        ]                                 # list length H\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, Seq_q, D)\n",
    "\n",
    "        out = self.out_proj(concat)  # cache the projection\n",
    "        # print(\"      [MultiHeadAttention] q:\", q.shape)\n",
    "        # print(\"      [MultiHeadAttention] k:\", k.shape)\n",
    "        # print(\"      [MultiHeadAttention] v:\", v.shape)\n",
    "        # print(\"      [MultiHeadAttention] output:\", out.shape)\n",
    "\n",
    "        return self.drop(out)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Synthetic 2×2–grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56×56   PatchSize: 4  → 14×14 = 196 patch tokens\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "MAX_DECODER_LEN  = 128                    #  some big number\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal mask \n",
    "def causal_mask(seq_len: int, device=None):\n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    mask = idx[:, None] >= idx[None, :]          # (T, T) bool\n",
    "    return mask[None, :, :]                      # (1, T, T)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * 4, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \"\"\"\n",
    "        x       : (B, T, D)   decoder input so far\n",
    "        enc_out : (B, S, D)   encoder memory\n",
    "        mask    : (1, 1, T, T) causal mask\n",
    "        \"\"\"\n",
    "        # 1. masked self-attention\n",
    "        qkv_in = self.ln1(x)\n",
    "        x = x + self.self_attn(qkv_in, qkv_in, qkv_in, mask)\n",
    "\n",
    "        # 2. encoder–decoder cross-attention\n",
    "        q = self.ln2(x)\n",
    "        k = v = enc_out                        # same tensor for key and value\n",
    "        x = x + self.cross_attn(q, k, v)       # no mask here\n",
    "\n",
    "        # 3. feed-forward\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 depth: int = 3, #number of decoder blocks stacked sequentially\n",
    "                 vocab_size: int = len(VOCAB)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size) \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, MAX_DECODER_LEN, hidden_size))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(hidden_size, num_heads, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, enc_out):\n",
    "        \"\"\"\n",
    "        targets : (B, T) integer ids including <start> at index 0\n",
    "        enc_out : (B, S, D)\n",
    "        returns\n",
    "        logits  : (B, T-1, vocab) – predictions for d1, d2, …, <finish>\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = targets.size()\n",
    "        \n",
    "        \n",
    "        token_inputs = targets[:, :-1] #(B, T-1)\n",
    "        # print(\"  token_inputs:\", token_inputs.shape)\n",
    "        x = self.token_embed(token_inputs) #(B, T-1, D)\n",
    "        # print(\"  embedded x:\", x.shape)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :] \n",
    "        # print(\"    [DecoderBlock] x input :\", x.shape)\n",
    "        \n",
    "        mask = causal_mask(T-1, x.device)        # (1,1,T-1,T-1)\n",
    "\n",
    "\n",
    "        #run through blocks \n",
    "        for blk in self.blocks: \n",
    "            x = blk(x, enc_out, mask) \n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        # print(\"  logits before return:\", logits.shape)\n",
    "        return logits \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTranscriber(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "        #reuse PatchEmbedding and Encoder \n",
    "        self.embed = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "        self.encoder = Encoder(NUM_ENCODER, EMBED_DIM, NUM_HEADS, DROPOUT)\n",
    "        self.decoder = DigitDecoder(EMBED_DIM, NUM_HEADS, depth = 3)\n",
    "        \n",
    "    def forward(self, images, targets): \n",
    "        # enc_seq = self.encoder(self.embed(images))\n",
    "        # logits = self.decoder(targets, enc_seq)\n",
    "        # return logits \n",
    "        # print(\"INPUT images shape:\", images.shape)\n",
    "        x = self.embed(images)\n",
    "        # print(\"AFTER PatchEmbedding:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(\"AFTER Encoder:\", x.shape)\n",
    "        logits = self.decoder(targets, x)\n",
    "        # print(\"AFTER Decoder:\", logits.shape)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "dummy_img = torch.randn(B, 1, 28, 28)          # batch of fake canvases\n",
    "dummy_tgt = torch.randint(0, 10, (B, 5))       # random digits 0-9\n",
    "model = GridTranscriber()\n",
    "out = model(dummy_img, dummy_tgt)\n",
    "# print(\"logits shape :\", out.shape)             # should be (2, 6, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GRID_SIZE    = 2                    # 2×2 cells\n",
    "CELL_PIX     = 14\n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX \n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # uint8 → 0-1\n",
    "    transforms.Normalize([0.5], [0.5])      # centre to −1..1\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels,\n",
    "                 epoch_size=60_000, rng=None):\n",
    "        self.base_images = base_images          # (N,28,28) uint8\n",
    "        self.base_labels = base_labels          # (N,)\n",
    "        self.epoch_size  = epoch_size\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "\n",
    "        # indices per digit for fast balanced sampling\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "        # 15 non-empty cell-occupancy patterns (bitmask 1..15)\n",
    "        patterns = np.arange(1, 16)\n",
    "        reps = math.ceil(epoch_size / len(patterns))\n",
    "        self.pattern_pool = self.rng.permutation(\n",
    "            np.tile(patterns, reps)[:epoch_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _cells_from_mask(mask: int):\n",
    "        return [i for i in range(4) if (mask >> i) & 1]   # TL,TR,BL,BR\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask     = int(self.pattern_pool[idx])\n",
    "        cell_ids = self._cells_from_mask(mask)\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        seq    = [VOCAB['<start>']]\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))\n",
    "            img_idx = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "            \n",
    "            # ↓↓ new: shrink 28×28 → 14×14 so 2×2 fits in 28×28\n",
    "            digit_img = TF.resize(Image.fromarray(digit_img), 14)\n",
    "            digit_img = np.array(digit_img, dtype=np.uint8)\n",
    "\n",
    "            row, col = divmod(cell, GRID_SIZE)\n",
    "            top, left = row*CELL_PIX, col*CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "\n",
    "            seq.append(VOCAB[str(d)])\n",
    "\n",
    "        seq.append(VOCAB['<finish>'])\n",
    "\n",
    "        return {\n",
    "            'image'  : grid_transforms(canvas),          # (1,56,56) float\n",
    "            'target' : torch.tensor(seq, dtype=torch.long),  # (6,)\n",
    "        }\n",
    "    \n",
    "# Collate function to add padding to the targets \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    images  = torch.stack([item['image'] for item in batch])  # shape: (B, 1, 56, 56)\n",
    "    targets = [item['target'] for item in batch]              # list of length-B tensors\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)  # (B, T_max)\n",
    "    return {'image': images, 'target': padded_targets}\n",
    "\n",
    "\n",
    "\n",
    "# ─── build train / val / test loaders ───────────────────────────────────────\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),   # we reuse MNIST test set\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# Decide how many CPU cores to devote to data loading.\n",
    "# Four to eight usually keeps the GPU fed without wasting resources.\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "NUM_WORKERS = min(8, os.cpu_count())        # 4-8 is a good starting range\n",
    "\n",
    "loader_kwargs = dict(\n",
    "    batch_size      = BATCH_SIZE,\n",
    "    num_workers     = NUM_WORKERS,          # <- key addition\n",
    "    pin_memory      = True,                 # speeds up host-to-device copy\n",
    "    persistent_workers = True,              # keeps workers alive across epochs\n",
    "    prefetch_factor = 4                     # each worker holds 4 batches ready\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_grid,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,       \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,           \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,      \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt  = ' '.join([str(VOCAB_INV[t]) for t in target if t != PAD_IDX])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Token Distribution (Training Set) ---\n",
      "       0: 12790\n",
      "       1: 12785\n",
      "       2: 12781\n",
      "       3: 12885\n",
      "       4: 12773\n",
      "       5: 12792\n",
      "       6: 12681\n",
      "       7: 12986\n",
      "       8: 12770\n",
      "       9: 12757\n",
      " <start>: 60000\n",
      "<finish>: 60000\n",
      "   <pad>: 0\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 1/10: 100%|██████████| 235/235 [01:26<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1 Validation Sample 0 --\n",
      "PRED (4 tokens): 8 8 6 <finish>\n",
      "GOLD (4 tokens): 4 3 9 <finish>\n",
      "-------------------------------------\n",
      "Epoch 1/10 │ train loss 1.9498 │ EM 0.0294 │ tok_acc 0.3570 │ val loss 1.6300 │ EM 0.0346 │ tok_acc 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 2/10: 100%|██████████| 235/235 [01:36<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 │ train loss 1.5828 │ EM 0.0409 │ tok_acc 0.4023 │ val loss 1.5579 │ EM 0.0485 │ tok_acc 0.4049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 3/10: 100%|██████████| 235/235 [01:53<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 │ train loss 1.5397 │ EM 0.0578 │ tok_acc 0.4202 │ val loss 1.5139 │ EM 0.0809 │ tok_acc 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 4/10: 100%|██████████| 235/235 [02:19<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 │ train loss 1.4716 │ EM 0.0889 │ tok_acc 0.4597 │ val loss 1.4055 │ EM 0.1110 │ tok_acc 0.4895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 5/10: 100%|██████████| 235/235 [02:04<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 │ train loss 1.2720 │ EM 0.1683 │ tok_acc 0.5500 │ val loss 1.0700 │ EM 0.2483 │ tok_acc 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 6/10: 100%|██████████| 235/235 [02:05<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 │ train loss 0.8268 │ EM 0.3867 │ tok_acc 0.7264 │ val loss 0.5727 │ EM 0.5462 │ tok_acc 0.8225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 7/10: 100%|██████████| 235/235 [01:58<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 │ train loss 0.4945 │ EM 0.5994 │ tok_acc 0.8455 │ val loss 0.4082 │ EM 0.6601 │ tok_acc 0.8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 8/10: 100%|██████████| 235/235 [02:17<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 │ train loss 0.3603 │ EM 0.6928 │ tok_acc 0.8887 │ val loss 0.3121 │ EM 0.7298 │ tok_acc 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 9/10: 100%|██████████| 235/235 [02:17<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 │ train loss 0.3014 │ EM 0.7406 │ tok_acc 0.9076 │ val loss 0.2721 │ EM 0.7688 │ tok_acc 0.9188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 10/10: 100%|██████████| 235/235 [02:27<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 │ train loss 0.2585 │ EM 0.7733 │ tok_acc 0.9208 │ val loss 0.2348 │ EM 0.7966 │ tok_acc 0.9294\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Optimizer & Loss\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "criterion = nn.CrossEntropyLoss()          # PAD will be masked manually\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    "    betas=ADAM_BETAS\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Exact‑match metric (ignores PAD)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def sequence_exact_match(pred_ids, gold_ids):\n",
    "    ok = (pred_ids == gold_ids) | (gold_ids == PAD_IDX)\n",
    "    return ok.all(dim=1).float().mean().item()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NEW: Print target token distribution\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n--- Target Token Distribution (Training Set) ---\")\n",
    "all_tokens = torch.cat([sample[\"target\"] for sample in train_grid]) # Assumes train_grid is loaded\n",
    "counts = torch.bincount(all_tokens, minlength=len(VOCAB)) # Assumes VOCAB is defined\n",
    "for i, c in enumerate(counts):\n",
    "    print(f\"{VOCAB_INV[i]:>8}: {c}\") # Assumes VOCAB_INV is defined\n",
    "print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Train / Validation loop\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "for epoch in range(EPOCHS):\n",
    "    # -------- TRAIN ------------------------------------------------------\n",
    "    model.train()\n",
    "    tr_loss = tr_em = tr_tok = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"train {epoch+1}/{EPOCHS}\"):\n",
    "        imgs    = batch[\"image\"].to(device)     # (B, 1, H, W)\n",
    "        targets = batch[\"target\"].to(device)    # (B, T)  <start>…<finish>\n",
    "\n",
    "        dec_in   = targets                      # (B, T)\n",
    "        gold_out = targets[:, 1:]               # (B, T‑1)\n",
    "\n",
    "        logits   = model(imgs, dec_in)          # (B, T‑1, V)\n",
    "        assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "        mask            = gold_out != PAD_IDX                     # (B, T‑1)\n",
    "        logits_flat     = logits.reshape(-1, len(VOCAB))          # (B*T, V)\n",
    "        gold_flat       = gold_out.reshape(-1)                    # (B*T,)\n",
    "\n",
    "        logits_masked   = logits_flat[mask.reshape(-1)]           # (~P, V)\n",
    "        gold_masked     = gold_flat[mask.reshape(-1)]             # (~P,)\n",
    "\n",
    "        loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds    = logits.argmax(dim=-1)\n",
    "\n",
    "        # ─── Token Accuracy (excluding PAD) ──────────────────────────────\n",
    "        tok_correct = (preds == gold_out) & mask\n",
    "        tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_em   += sequence_exact_match(preds, gold_out)\n",
    "        tr_tok  += tok_acc\n",
    "\n",
    "    tr_loss /= len(train_dataloader)\n",
    "    tr_em   /= len(train_dataloader)\n",
    "    tr_tok  /= len(train_dataloader)\n",
    "\n",
    "    # -------- VALIDATION -------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = val_em = val_tok = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            imgs    = batch[\"image\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            dec_in   = targets\n",
    "            gold_out = targets[:, 1:]\n",
    "\n",
    "            logits   = model(imgs, dec_in)\n",
    "            assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "            mask            = gold_out != PAD_IDX\n",
    "            logits_flat     = logits.reshape(-1, len(VOCAB))\n",
    "            gold_flat       = gold_out.reshape(-1)\n",
    "\n",
    "            logits_masked   = logits_flat[mask.reshape(-1)]\n",
    "            gold_masked     = gold_flat[mask.reshape(-1)]\n",
    "\n",
    "            loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "            preds    = logits.argmax(dim=-1)\n",
    "\n",
    "            # ─── Token Accuracy (excluding PAD) ──────────────────────────\n",
    "            tok_correct = (preds == gold_out) & mask\n",
    "            tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_em   += sequence_exact_match(preds, gold_out)\n",
    "            val_tok  += tok_acc\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_em   /= len(val_dataloader)\n",
    "    val_tok  /= len(val_dataloader)\n",
    "\n",
    "    # --- Sanity check: Visualize predictions vs gold for first epoch --- \n",
    "    if epoch == 0: \n",
    "        # Get the predictions and gold sequences from the *last* validation batch\n",
    "        # Note: .tolist() moves data from GPU to CPU if needed\n",
    "        idx = 0  # Index of the sample within the batch to visualize\n",
    "        if preds.shape[0] > idx: # Check if the batch has at least one sample\n",
    "            pred_seq = preds[idx].tolist()\n",
    "            gold_seq = gold_out[idx].tolist()\n",
    "            # Convert token IDs to strings, filtering out padding\n",
    "            pred_str = [VOCAB_INV[t] for t in pred_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            gold_str = [VOCAB_INV[t] for t in gold_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            print(f\"-- Epoch {epoch+1} Validation Sample {idx} --\")\n",
    "            print(f\"PRED ({len(pred_str)} tokens): {' '.join(pred_str)}\")\n",
    "            print(f\"GOLD ({len(gold_str)} tokens): {' '.join(gold_str)}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        else:\n",
    "            print(f\"-- Epoch {epoch+1} Validation: Last batch was empty or smaller than index {idx}, cannot show sample --\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} │ \"\n",
    "          f\"train loss {tr_loss:.4f} │ EM {tr_em:.4f} │ tok_acc {tr_tok:.4f} │ \"\n",
    "          f\"val loss {val_loss:.4f} │ EM {val_em:.4f} │ tok_acc {val_tok:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting greedy decode …\n",
      "step 1: 3\n",
      "step 2: 3\n",
      "step 3: 6\n",
      "step 4: 0\n",
      "step 5: 2\n",
      "step 6: <finish>\n",
      "final sequence: ['<start>', '3', '3', '6', '0', '2', '<finish>', '<pad>']\n",
      "Generated tokens: ['<start>', '3', '3', '6', '0', '2', '<finish>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# %% ──────────────────────────────────────────────────────────────────────────\n",
    "# Inference: greedy decoding on a single test image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0. Move model to the device you want to run on\n",
    "device = \"cpu\"        # ← change to \"cuda\" if you want GPU decoding\n",
    "model = model.to(device).eval()      # make sure this is your trained model\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Grab a single test image  (shape: 1×56×56)  and move to same device\n",
    "sample      = next(iter(test_dataloader))         # batch dict\n",
    "test_image  = sample[\"image\"][0].to(device)       # (1,56,56) tensor\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Minimal tokenizer wrapper around VOCAB / VOCAB_INV dicts\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab, vocab_inv):\n",
    "        self.token_to_idx = vocab\n",
    "        self.idx_to_token = vocab_inv\n",
    "\n",
    "tokenizer = SimpleTokenizer(VOCAB, VOCAB_INV)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Point to the patch‑embedding layer (lives in model.embed)\n",
    "patch_embed_layer = model.embed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  Greedy‑decode (function you defined earlier)\n",
    "def preprocess(image, patch_embed):\n",
    "    image = image.unsqueeze(0)                # (1,1,H,W)\n",
    "    return patch_embed(image)                 # (1, seq_len, D)\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    return [tokenizer.idx_to_token[i] for i in token_ids]\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, test_image, patch_embed, tokenizer, max_len=10):\n",
    "    print(\"Starting greedy decode …\")\n",
    "    model.eval()\n",
    "\n",
    "    encoder_input  = preprocess(test_image, patch_embed)\n",
    "    encoder_output = model.encoder(encoder_input)\n",
    "\n",
    "    start_id   = tokenizer.token_to_idx[\"<start>\"]\n",
    "    pad_id     = tokenizer.token_to_idx[\"<pad>\"]\n",
    "    finish_id  = tokenizer.token_to_idx[\"<finish>\"]\n",
    "\n",
    "    decoder_input = torch.full((1, max_len), pad_id, dtype=torch.long).to(test_image.device)\n",
    "    decoder_input[0, 0] = start_id\n",
    "\n",
    "    for t in range(1, max_len):\n",
    "        if t == 1:\n",
    "            # DigitDecoder will slice off last token, so we need at least 2\n",
    "            input_tokens = decoder_input[:, :2]\n",
    "        else:\n",
    "            input_tokens = decoder_input[:, :t]\n",
    "\n",
    "        logits = model.decoder(input_tokens, encoder_output)   # (1,t-1,vocab)\n",
    "        if logits.shape[1] == 0:\n",
    "            print(\"Warning: decoder returned empty logits at step\", t)\n",
    "            break\n",
    "\n",
    "        next_id = logits[:, -1].argmax(-1)  # (1,)\n",
    "        decoder_input[0, t] = next_id.item()\n",
    "\n",
    "        print(f\"step {t}: {tokenizer.idx_to_token[next_id.item()]}\")\n",
    "        if next_id.item() == finish_id:\n",
    "            break\n",
    "\n",
    "    tokens = decode_token_ids(decoder_input[0].tolist(), tokenizer)\n",
    "    print(\"final sequence:\", tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Run it\n",
    "generated = greedy_decode(\n",
    "    model       = model,\n",
    "    test_image  = test_image,\n",
    "    patch_embed = patch_embed_layer,\n",
    "    tokenizer   = tokenizer,\n",
    "    max_len     = 8        # adjust to your task’s longest sequence\n",
    ")\n",
    "\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting greedy decode …\n",
      "step 1: 4\n",
      "step 2: 4\n",
      "step 3: 7\n",
      "step 4: 6\n",
      "step 5: 7\n",
      "step 6: <finish>\n",
      "final sequence: ['<start>', '4', '4', '7', '6', '7', '<finish>', '<pad>']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟩 Ground Truth: ['<start>', '4', '7', '6', '7', '<finish>']\n",
      "🤖 Model Output: ['<start>', '4', '4', '7', '6', '7', '<finish>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grab batch and unpack\n",
    "sample = next(iter(test_dataloader))\n",
    "test_image = sample[\"image\"][0].to(device)   # (1, 56, 56)\n",
    "target_ids = sample[\"target\"][0].tolist()    # list of token IDs\n",
    "target_tokens = [VOCAB_INV[i] for i in target_ids if i != PAD_IDX]\n",
    "\n",
    "# Run model prediction\n",
    "generated_tokens = greedy_decode(\n",
    "    model       = model,\n",
    "    test_image  = test_image,\n",
    "    patch_embed = model.embed,\n",
    "    tokenizer   = tokenizer,\n",
    "    max_len     = 8\n",
    ")\n",
    "\n",
    "# Show the input image\n",
    "plt.imshow(test_image.squeeze().cpu(), cmap='gray')\n",
    "plt.title(\"Input 2×2 MNIST Grid\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Show sequences\n",
    "print(\"🟩 Ground Truth:\", target_tokens)\n",
    "print(\"🤖 Model Output:\", generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
