{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as TF   #  <-- add this line\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_PATCHES = 49\n"
     ]
    }
   ],
   "source": [
    "## Added Params (for training and testing)\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512 \n",
    "EPOCHS = 40 ##why this high number? usually for transformers you do 1,2,3. \n",
    "\n",
    "##\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "PATCH_SIZE = 4 \n",
    "IMAGE_SIZE = 28 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_HEADS = 8 #Within the transformer encoder there are attention heads- we choose 8 of them.                           \n",
    "DROPOUT = 0.001 \n",
    "HIDDEN_DIM = 768 #hidden dimentsion of MLP head for classification \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "\n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "NUM_ENCODER = 4 #stack encoders on top of each other (architecture just shows one)\n",
    "\n",
    "\n",
    "##This is the input size to the patch embedding layer (aka flattening image into sequence of patches )\n",
    "EMBED_DIM = 64 # 16 -> basically the number of patches\n",
    "\n",
    "## Paper defines the below as: N =HW / P^2\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"NUM_PATCHES =\", NUM_PATCHES) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERWRITE PARAMS TO TRAIN ON MY COMPUTER\n",
    "BATCH_SIZE = 256 #change to 256 if memory cant handle 512\n",
    "EPOCHS = 10\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 128\n",
    "NUM_ENCODER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "#function that divides images into patches\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) \n",
    "       \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches,embedding_dim)), requires_grad=True)  #Positional Embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        x = self.position_embedding + x #add posi\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# #always test model after you define it    \n",
    "# model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "# x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "# print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask=None):                                 # x: (B, S, D)\n",
    "        # q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        q = self.q_proj(q_in) # (B, Seq_q, d_h)\n",
    "        k = self.k_proj(k_in) # (B, Seq_k, d_h)\n",
    "        v = self.v_proj(v_in) # (B, Seq_v, d_h)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "            \n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.drop(attn)            # (B,S,S)\n",
    "        context = attn @ v \n",
    "        return context                                 # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):          \n",
    "        \"\"\"\n",
    "        q : (B, Seq_q, D)\n",
    "        k : (B, Seq_k, D)   defaults to q if None\n",
    "        v : (B, Seq_k, D)   defaults to k if None\n",
    "        mask : (1, 1, Seq_q, Seq_k) or None\n",
    "        \"\"\"\n",
    "        k = q if k is None else k\n",
    "        v = q if v is None else v\n",
    "\n",
    "        # run every head, collect their (B, Seq_q, d_h) outputs\n",
    "        head_outputs = [\n",
    "            head(q, k, v, mask) for head in self.heads\n",
    "        ]                                 # list length H\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, Seq_q, D)\n",
    "\n",
    "        out = self.out_proj(concat)  # cache the projection\n",
    "        # print(\"      [MultiHeadAttention] q:\", q.shape)\n",
    "        # print(\"      [MultiHeadAttention] k:\", k.shape)\n",
    "        # print(\"      [MultiHeadAttention] v:\", v.shape)\n",
    "        # print(\"      [MultiHeadAttention] output:\", out.shape)\n",
    "\n",
    "        return self.drop(out)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Synthetic 2×2–grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56×56   PatchSize: 4  → 14×14 = 196 patch tokens\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "MAX_DECODER_LEN  = 128                    #  some big number\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal mask \n",
    "def causal_mask(seq_len: int, device=None):\n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    mask = idx[:, None] >= idx[None, :]          # (T, T) bool\n",
    "    return mask[None, :, :]                      # (1, T, T)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * 4, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \"\"\"\n",
    "        x       : (B, T, D)   decoder input so far\n",
    "        enc_out : (B, S, D)   encoder memory\n",
    "        mask    : (1, 1, T, T) causal mask\n",
    "        \"\"\"\n",
    "        # 1. masked self-attention\n",
    "        qkv_in = self.ln1(x)\n",
    "        x = x + self.self_attn(qkv_in, qkv_in, qkv_in, mask)\n",
    "\n",
    "        # 2. encoder–decoder cross-attention\n",
    "        q = self.ln2(x)\n",
    "        k = v = enc_out                        # same tensor for key and value\n",
    "        x = x + self.cross_attn(q, k, v)       # no mask here\n",
    "\n",
    "        # 3. feed-forward\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 depth: int = 3, #number of decoder blocks stacked sequentially\n",
    "                 vocab_size: int = len(VOCAB)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size) \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, MAX_DECODER_LEN, hidden_size))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(hidden_size, num_heads, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, enc_out):\n",
    "        \"\"\"\n",
    "        targets : (B, T) integer IDs including <start> at index 0\n",
    "        enc_out : (B, S, D)\n",
    "        returns logits : (B, T-1, vocab) — predictions for d1, d2, ..., <finish>\n",
    "        \"\"\"\n",
    "        B, T = targets.size()\n",
    "\n",
    "        token_inputs = targets[:, :-1]  # Always slice for causal prediction\n",
    "\n",
    "        if token_inputs.size(1) == 0:\n",
    "            vocab_size = self.head.out_features\n",
    "            return torch.zeros(B, 0, vocab_size, device=targets.device)\n",
    "\n",
    "        x = self.token_embed(token_inputs)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        mask = causal_mask(x.size(1), x.device)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, enc_out, mask)\n",
    "\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTranscriber(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "        #reuse PatchEmbedding and Encoder \n",
    "        self.embed = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "        self.encoder = Encoder(NUM_ENCODER, EMBED_DIM, NUM_HEADS, DROPOUT)\n",
    "        self.decoder = DigitDecoder(EMBED_DIM, NUM_HEADS, depth = 3)\n",
    "        \n",
    "    def forward(self, images, targets): \n",
    "        # enc_seq = self.encoder(self.embed(images))\n",
    "        # logits = self.decoder(targets, enc_seq)\n",
    "        # return logits \n",
    "        # print(\"INPUT images shape:\", images.shape)\n",
    "        x = self.embed(images)\n",
    "        # print(\"AFTER PatchEmbedding:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(\"AFTER Encoder:\", x.shape)\n",
    "        logits = self.decoder(targets, x)\n",
    "        # print(\"AFTER Decoder:\", logits.shape)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "dummy_img = torch.randn(B, 1, 28, 28)          # batch of fake canvases\n",
    "dummy_tgt = torch.randint(0, 10, (B, 5))       # random digits 0-9\n",
    "model = GridTranscriber()\n",
    "out = model(dummy_img, dummy_tgt)\n",
    "# print(\"logits shape :\", out.shape)             # should be (2, 6, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GRID_SIZE    = 2                    # 2×2 cells\n",
    "CELL_PIX     = 14\n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX \n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # uint8 → 0-1\n",
    "    transforms.Normalize([0.5], [0.5])      # centre to −1..1\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels, epoch_size=60_000, rng=None):\n",
    "        self.base_images = base_images\n",
    "        self.base_labels = base_labels\n",
    "        self.epoch_size  = epoch_size\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "        # 🧠 Balanced digit counts\n",
    "        def _bitcount(x):\n",
    "            return (x & 1) + _bitcount(x >> 1) if x else 0\n",
    "\n",
    "        masks_by_len = {k: [m for m in range(1, 16) if _bitcount(m) == k] for k in range(1, 5)}\n",
    "        per_len = self.epoch_size // 4\n",
    "        pattern_pool = []\n",
    "\n",
    "        for k in range(1, 5):\n",
    "            pool_k = self.rng.choice(masks_by_len[k], size=per_len, replace=True)\n",
    "            pattern_pool.append(pool_k)\n",
    "\n",
    "        self.pattern_pool = self.rng.permutation(np.concatenate(pattern_pool)[:self.epoch_size])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _cells_from_mask(mask: int):\n",
    "        return [i for i in range(4) if (mask >> i) & 1]   # TL,TR,BL,BR\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask     = int(self.pattern_pool[idx])\n",
    "        cell_ids = self._cells_from_mask(mask)\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        seq    = [VOCAB['<start>']]\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))\n",
    "            img_idx = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "            \n",
    "            # ↓↓ new: shrink 28×28 → 14×14 so 2×2 fits in 28×28\n",
    "            digit_img = TF.resize(Image.fromarray(digit_img), 14)\n",
    "            digit_img = np.array(digit_img, dtype=np.uint8)\n",
    "\n",
    "            row, col = divmod(cell, GRID_SIZE)\n",
    "            top, left = row*CELL_PIX, col*CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "\n",
    "            seq.append(VOCAB[str(d)])\n",
    "\n",
    "        seq.append(VOCAB['<finish>'])\n",
    "\n",
    "        return {\n",
    "            'image'  : grid_transforms(canvas),          # (1,56,56) float\n",
    "            'target' : torch.tensor(seq, dtype=torch.long),  # (6,)\n",
    "        }\n",
    "    \n",
    "# Collate function to add padding to the targets \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    images  = torch.stack([item['image'] for item in batch])  # shape: (B, 1, 56, 56)\n",
    "    targets = [item['target'] for item in batch]              # list of length-B tensors\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)  # (B, T_max)\n",
    "    return {'image': images, 'target': padded_targets}\n",
    "\n",
    "\n",
    "\n",
    "# ─── build train / val / test loaders ───────────────────────────────────────\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),   # we reuse MNIST test set\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# Decide how many CPU cores to devote to data loading.\n",
    "# Four to eight usually keeps the GPU fed without wasting resources.\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "NUM_WORKERS = min(8, os.cpu_count())        # 4-8 is a good starting range\n",
    "\n",
    "loader_kwargs = dict(\n",
    "    batch_size      = BATCH_SIZE,\n",
    "    num_workers     = NUM_WORKERS,          # <- key addition\n",
    "    pin_memory      = True,                 # speeds up host-to-device copy\n",
    "    persistent_workers = True,              # keeps workers alive across epochs\n",
    "    prefetch_factor = 4                     # each worker holds 4 batches ready\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_grid,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,       \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,           \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,      \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt  = ' '.join([str(VOCAB_INV[t]) for t in target if t != PAD_IDX])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VISUALISE THE DATASET \n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "SAVE_DIR = \"./v3_debug_samples\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "N = 100  # how many samples to save\n",
    "\n",
    "for i in range(N):\n",
    "    sample = train_grid[i]  # uses your GridMNIST dataset\n",
    "    img = sample[\"image\"].squeeze(0)  # (56,56)\n",
    "    tgt = sample[\"target\"].tolist()\n",
    "    tokens = [VOCAB_INV[t] for t in tgt]\n",
    "\n",
    "    # Save image\n",
    "    img_pil = TF.to_pil_image(img * 0.5 + 0.5)  # unnormalize\n",
    "    img_pil.save(os.path.join(SAVE_DIR, f\"{i:03d}_{'-'.join(tokens)}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Digit Count Distribution:\n",
      "  1 digits: 15,000\n",
      "  2 digits: 15,000\n",
      "  3 digits: 15,000\n",
      "  4 digits: 15,000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "digit_counts = []\n",
    "\n",
    "for i in range(len(train_grid)):\n",
    "    target = train_grid[i]['target'].tolist()\n",
    "    tokens = [VOCAB_INV[t] for t in target]\n",
    "\n",
    "    try:\n",
    "        start = tokens.index('<start>')\n",
    "        finish = tokens.index('<finish>')\n",
    "        digits = tokens[start + 1 : finish]\n",
    "        digit_counts.append(len(digits))\n",
    "    except Exception:\n",
    "        print(\"Bad sample:\", tokens)\n",
    "\n",
    "# Count how many sequences have 1, 2, 3, 4 digits\n",
    "counts = Counter(digit_counts)\n",
    "\n",
    "# Print the exact numbers\n",
    "print(\"🧮 Digit Count Distribution:\")\n",
    "for num_digits in sorted(counts.keys()):\n",
    "    print(f\"  {num_digits} digits: {counts[num_digits]:,}\")\n",
    "\n",
    "# Plot the distribution\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.xlabel(\"Number of Digits per Sequence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Digit Count Distribution in Training Set\")\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Token Distribution (Training Set) ---\n",
      "       0: 14950\n",
      "       1: 14996\n",
      "       2: 15042\n",
      "       3: 14817\n",
      "       4: 15050\n",
      "       5: 15192\n",
      "       6: 15067\n",
      "       7: 14874\n",
      "       8: 15061\n",
      "       9: 14951\n",
      " <start>: 60000\n",
      "<finish>: 60000\n",
      "   <pad>: 0\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 1/10: 100%|██████████| 235/235 [01:34<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1 Validation Sample 0 --\n",
      "PRED (5 tokens): 1 <finish> <finish> <finish> <finish>\n",
      "GOLD (2 tokens): 5 <finish>\n",
      "-------------------------------------\n",
      "Epoch 1/10 │ train loss 2.0034 │ EM 0.0276 │ tok_acc 0.3242 │ val loss 1.6899 │ EM 0.0316 │ tok_acc 0.3639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 2/10: 100%|██████████| 235/235 [01:37<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 │ train loss 1.6569 │ EM 0.0338 │ tok_acc 0.3714 │ val loss 1.6384 │ EM 0.0394 │ tok_acc 0.3689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 3/10: 100%|██████████| 235/235 [01:41<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 │ train loss 1.6206 │ EM 0.0518 │ tok_acc 0.3873 │ val loss 1.5970 │ EM 0.0709 │ tok_acc 0.4006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 4/10: 100%|██████████| 235/235 [01:43<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 │ train loss 1.5562 │ EM 0.0761 │ tok_acc 0.4261 │ val loss 1.4798 │ EM 0.0962 │ tok_acc 0.4596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 5/10: 100%|██████████| 235/235 [01:45<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 │ train loss 1.3693 │ EM 0.1306 │ tok_acc 0.5098 │ val loss 1.1834 │ EM 0.1989 │ tok_acc 0.5861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 6/10: 100%|██████████| 235/235 [02:26<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 │ train loss 0.9099 │ EM 0.3215 │ tok_acc 0.6959 │ val loss 0.6819 │ EM 0.4353 │ tok_acc 0.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 7/10: 100%|██████████| 235/235 [01:26<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 │ train loss 0.5435 │ EM 0.5371 │ tok_acc 0.8297 │ val loss 0.4343 │ EM 0.6212 │ tok_acc 0.8684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 8/10:  60%|██████    | 142/235 [08:48<00:50,  1.84it/s] "
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Optimizer & Loss\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "criterion = nn.CrossEntropyLoss()          # PAD will be masked manually\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    "    betas=ADAM_BETAS\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Exact‑match metric (ignores PAD)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def sequence_exact_match(pred_ids, gold_ids):\n",
    "    ok = (pred_ids == gold_ids) | (gold_ids == PAD_IDX)\n",
    "    return ok.all(dim=1).float().mean().item()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NEW: Print target token distribution\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n--- Target Token Distribution (Training Set) ---\")\n",
    "all_tokens = torch.cat([sample[\"target\"] for sample in train_grid]) # Assumes train_grid is loaded\n",
    "counts = torch.bincount(all_tokens, minlength=len(VOCAB)) # Assumes VOCAB is defined\n",
    "for i, c in enumerate(counts):\n",
    "    print(f\"{VOCAB_INV[i]:>8}: {c}\") # Assumes VOCAB_INV is defined\n",
    "print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Train / Validation loop\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "for epoch in range(EPOCHS):\n",
    "    # -------- TRAIN ------------------------------------------------------\n",
    "    model.train()\n",
    "    tr_loss = tr_em = tr_tok = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"train {epoch+1}/{EPOCHS}\"):\n",
    "        imgs    = batch[\"image\"].to(device)     # (B, 1, H, W)\n",
    "        targets = batch[\"target\"].to(device)    # (B, T)  <start>…<finish>\n",
    "\n",
    "        dec_in   = targets                      # (B, T)\n",
    "        gold_out = targets[:, 1:]               # (B, T‑1)\n",
    "\n",
    "        logits   = model(imgs, dec_in)          # (B, T‑1, V)\n",
    "        assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "        mask            = gold_out != PAD_IDX                     # (B, T‑1)\n",
    "        logits_flat     = logits.reshape(-1, len(VOCAB))          # (B*T, V)\n",
    "        gold_flat       = gold_out.reshape(-1)                    # (B*T,)\n",
    "\n",
    "        logits_masked   = logits_flat[mask.reshape(-1)]           # (~P, V)\n",
    "        gold_masked     = gold_flat[mask.reshape(-1)]             # (~P,)\n",
    "\n",
    "        loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds    = logits.argmax(dim=-1)\n",
    "\n",
    "        # ─── Token Accuracy (excluding PAD) ──────────────────────────────\n",
    "        tok_correct = (preds == gold_out) & mask\n",
    "        tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_em   += sequence_exact_match(preds, gold_out)\n",
    "        tr_tok  += tok_acc\n",
    "\n",
    "    tr_loss /= len(train_dataloader)\n",
    "    tr_em   /= len(train_dataloader)\n",
    "    tr_tok  /= len(train_dataloader)\n",
    "\n",
    "    # -------- VALIDATION -------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = val_em = val_tok = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            imgs    = batch[\"image\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            dec_in   = targets\n",
    "            gold_out = targets[:, 1:]\n",
    "\n",
    "            logits   = model(imgs, dec_in)\n",
    "            assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "            mask            = gold_out != PAD_IDX\n",
    "            logits_flat     = logits.reshape(-1, len(VOCAB))\n",
    "            gold_flat       = gold_out.reshape(-1)\n",
    "\n",
    "            logits_masked   = logits_flat[mask.reshape(-1)]\n",
    "            gold_masked     = gold_flat[mask.reshape(-1)]\n",
    "\n",
    "            loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "            preds    = logits.argmax(dim=-1)\n",
    "\n",
    "            # ─── Token Accuracy (excluding PAD) ──────────────────────────\n",
    "            tok_correct = (preds == gold_out) & mask\n",
    "            tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_em   += sequence_exact_match(preds, gold_out)\n",
    "            val_tok  += tok_acc\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_em   /= len(val_dataloader)\n",
    "    val_tok  /= len(val_dataloader)\n",
    "\n",
    "    # --- Sanity check: Visualize predictions vs gold for first epoch --- \n",
    "    if epoch == 0: \n",
    "        # Get the predictions and gold sequences from the *last* validation batch\n",
    "        # Note: .tolist() moves data from GPU to CPU if needed\n",
    "        idx = 0  # Index of the sample within the batch to visualize\n",
    "        if preds.shape[0] > idx: # Check if the batch has at least one sample\n",
    "            pred_seq = preds[idx].tolist()\n",
    "            gold_seq = gold_out[idx].tolist()\n",
    "            # Convert token IDs to strings, filtering out padding\n",
    "            pred_str = [VOCAB_INV[t] for t in pred_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            gold_str = [VOCAB_INV[t] for t in gold_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            print(f\"-- Epoch {epoch+1} Validation Sample {idx} --\")\n",
    "            print(f\"PRED ({len(pred_str)} tokens): {' '.join(pred_str)}\")\n",
    "            print(f\"GOLD ({len(gold_str)} tokens): {' '.join(gold_str)}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        else:\n",
    "            print(f\"-- Epoch {epoch+1} Validation: Last batch was empty or smaller than index {idx}, cannot show sample --\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} │ \"\n",
    "          f\"train loss {tr_loss:.4f} │ EM {tr_em:.4f} │ tok_acc {tr_tok:.4f} │ \"\n",
    "          f\"val loss {val_loss:.4f} │ EM {val_em:.4f} │ tok_acc {val_tok:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ──────────────────────────────────────────────────────────────────────────\n",
    "# Greedy decode + quick visual sanity‑check\n",
    "import os, torch, matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device = torch.device(\"cpu\")        # or \"cuda\"\n",
    "model  = model.to(device).eval()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def preprocess(img, patch_embed):\n",
    "    \"\"\"img : (1,56,56) → (1,seq,D)\"\"\"\n",
    "    return patch_embed(img.unsqueeze(0))\n",
    "\n",
    "def greedy_decode(model, img, patch_embed, tokenizer, max_len=8):\n",
    "    \"\"\"\n",
    "    img        : (1,56,56) tensor, values in [-1,1]\n",
    "    patch_embed: model.embed   (patchifier)\n",
    "    returns list of *clean* digit tokens  (no specials)\n",
    "    \"\"\"\n",
    "    mem = model.encoder(preprocess(img, patch_embed))  # (1,S,D)\n",
    "\n",
    "    start_id  = tokenizer.token_to_idx[\"<start>\"]\n",
    "    finish_id = tokenizer.token_to_idx[\"<finish>\"]\n",
    "    pad_id    = tokenizer.token_to_idx[\"<pad>\"]\n",
    "\n",
    "    ys = torch.tensor([[start_id, pad_id]], device=img.device)  # seed <start> <pad>\n",
    "\n",
    "    for step in range(max_len - 1):                 # already have 2 slots\n",
    "        logits = model.decoder(ys, mem)             # (1,t,vocab)  t ≥ 1\n",
    "\n",
    "        # Prevent predicting <finish> right after <start>\n",
    "        # This logic might need adjustment based on when exactly we want to prevent it.\n",
    "        # If we only want to prevent it on the *very first* prediction step:\n",
    "        if ys.shape[1] == 2: # If sequence length is exactly 2 (<start>, <pad>)\n",
    "             logits[:, -1, finish_id] = -float('inf')\n",
    "        # If we want to prevent it anytime before a non-special token is generated,\n",
    "        # the logic would be more complex. Sticking to the simple case for now.\n",
    "\n",
    "        next_id = logits[:, -1].argmax(-1)          # (1,)\n",
    "        ys[0, -1] = next_id                         # overwrite the dummy pad\n",
    "\n",
    "        if next_id.item() == finish_id:\n",
    "            break                                   # finished -> stop expanding\n",
    "\n",
    "        # Append a *new* pad token for the next prediction step\n",
    "        ys = torch.cat([ys, torch.tensor([[pad_id]], device=img.device)], dim=1)\n",
    "\n",
    "    # strip specials\n",
    "    clean = [i for i in ys[0].tolist()\n",
    "             if i not in {start_id, finish_id, pad_id}]\n",
    "    return [tokenizer.idx_to_token[i] for i in clean]\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Quick demo on first 50 test samples + optional PNG dump\n",
    "def dump_preds(n=50, out_dir=\"./model_outputs\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"🔍 Dumping model predictions…\")\n",
    "\n",
    "    for idx in range(n):\n",
    "        sample = test_grid[idx]\n",
    "        img    = sample['image'].to(device)\n",
    "        gold   = [VOCAB_INV[i] for i in sample['target'].tolist()\n",
    "                  if i not in {PAD_IDX, VOCAB['<start>'], VOCAB['<finish>']}]\n",
    "\n",
    "        pred   = greedy_decode(model, img, model.embed, tokenizer, max_len=10)\n",
    "\n",
    "        print(f\"[{idx:03d}]  True: {' '.join(gold)}\")\n",
    "        print(f\"      Pred: {' '.join(pred)}\")\n",
    "\n",
    "        png = TF.to_pil_image(img.squeeze().cpu()*0.5 + 0.5)\n",
    "        fname = f\"{idx:03d}_gold={'-'.join(gold)}_pred={'-'.join(pred)}.png\"\n",
    "        png.save(os.path.join(out_dir, fname))\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "\n",
    "# — run the dump —\n",
    "dump_preds(n=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Grab batch and unpack\n",
    "sample = next(iter(test_dataloader))\n",
    "test_image = sample[\"image\"][0].to(device)   # (1, 56, 56)\n",
    "target_ids = sample[\"target\"][0].tolist()    # list of token IDs\n",
    "target_tokens = [VOCAB_INV[i] for i in target_ids if i != PAD_IDX]\n",
    "\n",
    "# Run model prediction\n",
    "generated_tokens = greedy_decode(\n",
    "    model       = model,\n",
    "    test_image  = test_image,\n",
    "    patch_embed = model.embed,\n",
    "    tokenizer   = tokenizer,\n",
    "    max_len     = 8\n",
    ")\n",
    "\n",
    "# Show the input image\n",
    "plt.imshow(test_image.squeeze().cpu(), cmap='gray')\n",
    "plt.title(\"Input 2×2 MNIST Grid\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Show sequences\n",
    "print(\"🟩 Ground Truth:\", target_tokens)\n",
    "print(\"🤖 Model Output:\", generated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model_outputs(n=50, save_dir=\"./model_outputs\"):\n",
    "    print(\"🔍 Dumping model predictions...\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(n):\n",
    "        sample = test_grid[i]\n",
    "        image = sample[\"image\"].to(device)\n",
    "        target = sample[\"target\"].tolist()\n",
    "\n",
    "        gold_tokens = [VOCAB_INV[t] for t in target if t not in {PAD_IDX, VOCAB['<start>'], VOCAB['<finish>']}]\n",
    "\n",
    "        try:\n",
    "            pred = greedy_decode(model, image, model.embed, tokenizer, max_len=10)\n",
    "            pred_tokens = [t for t in pred if t not in {\"<start>\", \"<finish>\", \"<pad>\"}]\n",
    "        except Exception as e:\n",
    "            print(f\"[{i:03d}] ❌ Decode failed:\", e)\n",
    "            continue\n",
    "\n",
    "        print(f\"[{i:03d}]  True: {' '.join(gold_tokens)}\")\n",
    "        print(f\"      Pred: {' '.join(pred_tokens)}\")\n",
    "\n",
    "        # Save image\n",
    "        try:\n",
    "            img_pil = TF.to_pil_image(image.squeeze().cpu() * 0.5 + 0.5)\n",
    "            filename = f\"{i:03d}_gold={'-'.join(gold_tokens)}_pred={'-'.join(pred_tokens)}.png\"\n",
    "            img_pil.save(os.path.join(save_dir, filename))\n",
    "        except Exception as e:\n",
    "            print(f\"[{i:03d}] ❌ Save failed:\", e)\n",
    "\n",
    "    print(\"✅ Done.\")\n",
    "\n",
    "dump_model_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
