{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "PATCH_SIZE = 4 #we chose 4-> pixel length of 1 dimension\n",
    "IMAGE_SIZE = 28 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_HEADS = 8 #Within the transformer encoder there are attention heads- we choose 8 of them.                           \n",
    "DROPOUT = 0.001 \n",
    "HIDDEN_DIM = 768 #hidden dimentsion of MLP head for classification \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "\n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "NUM_ENCODER = 4 #stack encoders on top of each other (architecture just shows one)\n",
    "\n",
    "\n",
    "##This is the input size to the patch embedding layer (aka flattening image into sequence of patches )\n",
    "EMBED_DIM = (PATCH_SIZE**2) * IN_CHANNELS # 16 -> basically the number of patches\n",
    "\n",
    "## Paper defines the below as: N =HW / P^2\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "device = \"cude\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Check in on [CLS] Token and Positional Embeddings \n",
    "\n",
    "In ViT: \n",
    "- split image into patches and turn them into embeddings (1 patch = 1 embedding vector)\n",
    "- model pretends that the patches are a sequence of tokens, just like words in NLP models like BERT\n",
    "\n",
    "2 Important points : \n",
    "1) prepend a special [CLS] token (like \"classification token\") at the start of the sequence.\n",
    "2) add positional embeddings to every token (patches and the [CLS] token) so the model knows the order.\n",
    "\n",
    "# Example:\n",
    "1) After patch embedding, suppose we have: \n",
    "- 100 patches, where each patch is an embedding vector of size D (say, 768)\n",
    "- so our sequence has shape: (100,768)\n",
    "\n",
    "2) Create [CLS] token \n",
    "- Create a new learnable vector (randomly initialized) of size D, called the [CLS] token -> just another vector like a patch but it doesn't come from the image\n",
    "\n",
    "3) Prepend the [CLS] token\n",
    "- now sequence becomes: (1+100,768)\n",
    "- where First position: [CLS] token & Next positions: patch tokens\n",
    "\n",
    "4) Add Positional Embeddings \n",
    "- Transformers have no sense of order natively, so you add (element-wise) a positional embedding vector to each token\n",
    "- The [CLS] token gets a positional embedding for position 0.\n",
    "-  Patch tokens get positional embeddings for positions 1, 2, 3, ..., 100.\n",
    "- Now the model knows which patch is where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 50, 16])\n"
     ]
    }
   ],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        #function that divides images into patches\n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) #equivalent to nn.Flatten(start_dim=2, end_dim=-1) -> not a learnable layer (converts patched into sequence of vectors)\n",
    "        \n",
    "            #OUTPUT SHAPE: (batch_size, embedding_dim, num_patches) AKA the full sequence of patches\n",
    "            \n",
    "        \n",
    "        #---- CLS Token ---- \n",
    "     \n",
    "        #here we define the [CLS] token. nn.Parameter is a learnable tensor (its a single parameter not a full layer)\n",
    "        # Create a random tensor of shape (1, in_channels, embedding_dim), wrap it as a learnable parameter, and assign it as the CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embedding_dim)), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        #---- Positional Embedding ---- \n",
    "        \n",
    "        \n",
    "        #positional embedding is a learnable parameter \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches+1,embedding_dim)), requires_grad=True) #we add 1 to num_patches because we have the [CLS] token\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    #after patching and flattening we have a tesnor of shape (batch_size, embedding_dim, num_patches) e.g., (32, 16, 49)\n",
    "    # x = x.permute(0, 2, 1) rearranges to (batch_size, num_patches, embedding_dim) e.g., (32, 49, 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        #here we expand the cls token so its not just the shape for 1 sample but for a batch of images\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) #expand the cls token to the batch size. x.shape[0] is the batch size. -1, -1 tells expand function to keep original dimensions. \n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        \n",
    "        #1 axis for batches, 1 axis for sequence of patches, 1 axis for embedding dimension \n",
    "        x = torch.cat([cls_token, x], dim=1) #so we want to add the CLS token to the left of the patches\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#always test model after you define it    \n",
    "model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 28, 28) #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: nn.Sequential is a convenience container in PyTorch.\n",
    "It lets you stack layers together in order, without writing a full forward() method manually.\n",
    "\n",
    "Instead of writing:\n",
    " ```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "```\n",
    "You can do the same thing with nn.Sequential:\n",
    "\n",
    "```python\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the layers:\n",
    "\n",
    "-  nn.Conv2d is a layer in PyTorch (torch.nn) used for applying a 2D convolution over an input, typically an image.\n",
    "    It slides filters (kernels) over a 2D input (like an image) and computes feature maps.\n",
    "- nn.Flatten() reshapes a tensor by flattening part of its dimensions into a single one.\n",
    "     Turns multi-dimensional data (like 2D or 3D feature maps) into a 1D vector per sample, usually before feeding it into fully connected (Linear) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on cls_token:\n",
    "- shape is: 1 × in_channels × embedding_dim\n",
    "    - 1 -> Becaues we only have one CLS token per sample\n",
    "    - in_channels × embedding_dim -> to match the dimensions of the patch embedding vector because we add it to the sequence before feeding into the transformer.\n",
    "    - another note on the 1, it is a batch-size like-dimenssion, and we replace it with the batch size\n",
    "    \n",
    "    \n",
    "- why have it? \n",
    "    - CLS token = Learnable summary of the whole input.\n",
    "    - It acts as a summary token: after going through the transformer layers, the model will read the CLS token to decide the final class label.\n",
    "    - Think of it like a \"learnable prompt\" — the model writes its summary into it during training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking the shapes:\n",
    "\n",
    "You start with an image of shape:\n",
    "(batch_size, in_channels, height, width) = (32, 1, 28, 28)\n",
    "\n",
    "\n",
    "then we apply patcher \n",
    "\n",
    "```\n",
    "self.patcher = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, embedding_dim, patch_size, stride=patch_size),\n",
    "    nn.Flatten(start_dim=2)\n",
    ")\n",
    "\n",
    "```\n",
    "where nn.Conv2D divides the image into patches of size 4×4 pixels so we get: \n",
    "(batch_size, embedding_dim, height//patch_size, width//patch_size) = (32, 16, 7, 7)\n",
    "     where: \n",
    "     - 28 // 4 = 7 patches along height\n",
    "     - 28 // 4 = 7 patches along width\n",
    "     - 16 filters = 16 features per patch\n",
    "    \n",
    "Apply nn.Flatten(start_dim=2), which Flatten from dimension 2 onward:\n",
    "- Flatten (7,7) together into 49, so after flattening:\n",
    "- (batch_size, embedding_dim, num_patches) = (32, 16, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick notes on below:\n",
    "- We implemented our encoder block using pytorch but we're meant to explicitely code it out -> I will do this after doing one full iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit(nn.Module):\n",
    "    def __init__(self, num_patches, num_classes, patch_size, embed_dim, num_encoders, num_heads, hidden_dim, dropout, activation, in_channels):\n",
    "        super().__init__()\n",
    "        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels) #call the class we trained earlier -> this will give us the input to our encoder (divide image into patches and generate sequences)\n",
    "        \n",
    "        \n",
    "        #---- ENCODER ---- \n",
    "        #PyTorch version:\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=True) # we defined our images so that batch size comes first, so we should add: batch_first=True \n",
    "        \n",
    "        #above is only one encoder layer, we're stacking many encoder layers:\n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "        \n",
    "        #---- MLP HEAD ---- \n",
    "        # The ViT typically uses only the [CLS] token for classification\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim) #normalise each sequence with itself \n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes) #since we are doing classification, output features is number of classes (10 in our case)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x - self.mlp_head(x[:, 0, :]) #we dont classify the whole embedding, instead we classify the CLS token in the beginning because its a learnable parameter and its meant to contain all the information the other parameters have\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
