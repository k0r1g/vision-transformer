{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Added Params (for training and testing)\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 512 \n",
    "EPOCHS = 40 ##why this high number? usually for transformers you do 1,2,3. \n",
    "\n",
    "##\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "PATCH_SIZE = 4 #we chose 4-> pixel length of 1 dimension\n",
    "IMAGE_SIZE = 28 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_HEADS = 8 #Within the transformer encoder there are attention heads- we choose 8 of them.                           \n",
    "DROPOUT = 0.001 \n",
    "HIDDEN_DIM = 768 #hidden dimentsion of MLP head for classification \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "\n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "NUM_ENCODER = 4 #stack encoders on top of each other (architecture just shows one)\n",
    "\n",
    "\n",
    "##This is the input size to the patch embedding layer (aka flattening image into sequence of patches )\n",
    "EMBED_DIM = 64 # 16 -> basically the number of patches\n",
    "\n",
    "## Paper defines the below as: N =HW / P^2\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cude\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERWRITE PARAMS TO TRAIN ON MY COMPUTER\n",
    "BATCH_SIZE = 512 #change to 256 if memory cant handle 512\n",
    "EPOCHS = 6\n",
    "NUM_HEADS = 4\n",
    "HIDDEN_DIM = 128\n",
    "NUM_ENCODER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Check in on [CLS] Token and Positional Embeddings \n",
    "\n",
    "In ViT: \n",
    "- split image into patches and turn them into embeddings (1 patch = 1 embedding vector)\n",
    "- model pretends that the patches are a sequence of tokens, just like words in NLP models like BERT\n",
    "\n",
    "2 Important points : \n",
    "1) prepend a special [CLS] token (like \"classification token\") at the start of the sequence.\n",
    "2) add positional embeddings to every token (patches and the [CLS] token) so the model knows the order.\n",
    "\n",
    "# Example:\n",
    "1) After patch embedding, suppose we have: \n",
    "- 100 patches, where each patch is an embedding vector of size D (say, 768)\n",
    "- so our sequence has shape: (100,768)\n",
    "\n",
    "2) Create [CLS] token \n",
    "- Create a new learnable vector (randomly initialized) of size D, called the [CLS] token -> just another vector like a patch but it doesn't come from the image\n",
    "\n",
    "3) Prepend the [CLS] token\n",
    "- now sequence becomes: (1+100,768)\n",
    "- where First position: [CLS] token & Next positions: patch tokens\n",
    "\n",
    "4) Add Positional Embeddings \n",
    "- Transformers have no sense of order natively, so you add (element-wise) a positional embedding vector to each token\n",
    "- The [CLS] token gets a positional embedding for position 0.\n",
    "-  Patch tokens get positional embeddings for positions 1, 2, 3, ..., 100.\n",
    "- Now the model knows which patch is where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        #function that divides images into patches\n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) #equivalent to nn.Flatten(start_dim=2, end_dim=-1) -> not a learnable layer (converts patched into sequence of vectors)\n",
    "        \n",
    "            #OUTPUT SHAPE: (batch_size, embedding_dim, num_patches) AKA the full sequence of patches\n",
    "            \n",
    "        \n",
    "        #---- CLS Token ---- \n",
    "     \n",
    "        #here we define the [CLS] token. nn.Parameter is a learnable tensor (its a single parameter not a full layer)\n",
    "        # Create a random tensor of shape (1, in_channels, embedding_dim), wrap it as a learnable parameter, and assign it as the CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embedding_dim)), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        #---- Positional Embedding ---- \n",
    "        \n",
    "        \n",
    "        #positional embedding is a learnable parameter \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches+1,embedding_dim)), requires_grad=True) #we add 1 to num_patches because we have the [CLS] token\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    #after patching and flattening we have a tesnor of shape (batch_size, embedding_dim, num_patches) e.g., (32, 16, 49)\n",
    "    # x = x.permute(0, 2, 1) rearranges to (batch_size, num_patches, embedding_dim) e.g., (32, 49, 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        #here we expand the cls token so its not just the shape for 1 sample but for a batch of images\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) #expand the cls token to the batch size. x.shape[0] is the batch size. -1, -1 tells expand function to keep original dimensions. \n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        \n",
    "        #1 axis for batches, 1 axis for sequence of patches, 1 axis for embedding dimension \n",
    "        x = torch.cat([cls_token, x], dim=1) #so we want to add the CLS token to the left of the patches\n",
    "        \n",
    "        #then we need to add the position tokens to each patch \n",
    "        x = self.position_embedding + x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "#always test model after you define it    \n",
    "model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 28, 28).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: nn.Sequential is a convenience container in PyTorch.\n",
    "It lets you stack layers together in order, without writing a full forward() method manually.\n",
    "\n",
    "Instead of writing:\n",
    " ```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(20, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "```\n",
    "You can do the same thing with nn.Sequential:\n",
    "\n",
    "```python\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the layers:\n",
    "\n",
    "-  nn.Conv2d is a layer in PyTorch (torch.nn) used for applying a 2D convolution over an input, typically an image.\n",
    "    It slides filters (kernels) over a 2D input (like an image) and computes feature maps.\n",
    "- nn.Flatten() reshapes a tensor by flattening part of its dimensions into a single one.\n",
    "     Turns multi-dimensional data (like 2D or 3D feature maps) into a 1D vector per sample, usually before feeding it into fully connected (Linear) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on cls_token:\n",
    "- shape is: 1 × in_channels × embedding_dim\n",
    "    - 1 -> Becaues we only have one CLS token per sample\n",
    "    - in_channels × embedding_dim -> to match the dimensions of the patch embedding vector because we add it to the sequence before feeding into the transformer.\n",
    "    - another note on the 1, it is a batch-size like-dimenssion, and we replace it with the batch size\n",
    "    \n",
    "    \n",
    "- why have it? \n",
    "    - CLS token = Learnable summary of the whole input.\n",
    "    - It acts as a summary token: after going through the transformer layers, the model will read the CLS token to decide the final class label.\n",
    "    - Think of it like a \"learnable prompt\" — the model writes its summary into it during training.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking the shapes:\n",
    "\n",
    "You start with an image of shape:\n",
    "(batch_size, in_channels, height, width) = (32, 1, 28, 28)\n",
    "\n",
    "\n",
    "then we apply patcher \n",
    "\n",
    "```\n",
    "self.patcher = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, embedding_dim, patch_size, stride=patch_size),\n",
    "    nn.Flatten(start_dim=2)\n",
    ")\n",
    "\n",
    "```\n",
    "where nn.Conv2D divides the image into patches of size 4×4 pixels so we get: \n",
    "(batch_size, embedding_dim, height//patch_size, width//patch_size) = (32, 16, 7, 7)\n",
    "     where: \n",
    "     - 28 // 4 = 7 patches along height\n",
    "     - 28 // 4 = 7 patches along width\n",
    "     - 16 filters = 16 features per patch\n",
    "    \n",
    "Apply nn.Flatten(start_dim=2), which Flatten from dimension 2 onward:\n",
    "- Flatten (7,7) together into 49, so after flattening:\n",
    "- (batch_size, embedding_dim, num_patches) = (32, 16, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoder block\n",
    "Recall the below:\n",
    "![vit_architecture](ViT_architecture.png)\n",
    "- Nx = encoder is repeated N times \n",
    "- Multi-head attention has 3 inputs (queries, keys, values)\n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Quick notes on below:\n",
    "- We implemented our encoder block using pytorch but we're meant to explicitely code it out -> I will do this after doing one full iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B, S, D)\n",
    "        q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        attn = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = self.drop(attn.softmax(dim=-1))            # (B,S,S)\n",
    "        return attn @ v                                   # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden dim not divisible by heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B,S,D)\n",
    "        # list of (B,S,d_h) → cat on last dim → (B,S,D)\n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.drop(self.out_proj(x))                # (B,S,D)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n",
    "\n",
    "# === 6. ViT classifier head (uses Encoder) ====================================\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_patches: int = NUM_PATCHES,\n",
    "                 num_classes: int = NUM_CLASSES,\n",
    "                 patch_size: int = PATCH_SIZE,\n",
    "                 embed_dim: int = EMBED_DIM,\n",
    "                 depth: int = NUM_ENCODER,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 in_channels: int = IN_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.embed = PatchEmbedding(embed_dim, patch_size,\n",
    "                                    num_patches, dropout, in_channels)\n",
    "        self.encoder = Encoder(depth, embed_dim, num_heads, dropout)\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B,C,H,W)\n",
    "        x = self.embed(x)                                 # (B,S,D)\n",
    "        x = self.encoder(x)                               # (B,S,D)\n",
    "        cls = x[:, 0]                                     # (B,D)\n",
    "        return self.cls_head(cls)                         # (B,num_classes)\n",
    "\n",
    "model = ViT(NUM_PATCHES, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODER, NUM_HEADS, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 28, 28) #dummy image\n",
    "print(model(x).shape) #expect [512, 10] -> batch of 512 (512 images) for 10 classes -> returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the flow is:\n",
    "Transformer Encoder outputs (batch_size, num_patches + 1, embed_dim).\n",
    "\n",
    "You select the first token (CLS):\n",
    "```\n",
    "cls_token = output[:, 0, :]\n",
    "```\n",
    "→ shape (batch_size, embed_dim)\n",
    "\n",
    "Then apply:\n",
    "\n",
    "LayerNorm (normalize each embed_dim vector independently)\n",
    "\n",
    "Linear (map to num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "#Convert to Pandas dataframe \n",
    "# Train\n",
    "train_images = train_dataset.data.view(-1, 28*28).numpy()  # flatten images\n",
    "train_labels = train_dataset.targets.numpy()               # labels\n",
    "\n",
    "train_df = pd.DataFrame(train_images)\n",
    "train_df['label'] = train_labels  # add labels at the end\n",
    "\n",
    "# Test\n",
    "test_images = test_dataset.data.view(-1, 28*28).numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "test_df = pd.DataFrame(test_images)\n",
    "test_df['label'] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  781  782  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   783  label  \n",
       "0    0      5  \n",
       "1    0      0  \n",
       "2    0      4  \n",
       "3    0      1  \n",
       "4    0      9  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  781  782  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
       "\n",
       "   783  label  \n",
       "0    0      7  \n",
       "1    0      2  \n",
       "2    0      1  \n",
       "3    0      0  \n",
       "4    0      4  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data into train and validation sets \n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need 3 separate dataset objects: train, val, test \n",
    "\n",
    "\n",
    "#---- TRAIN DATASET ---- \n",
    "\n",
    "class MNISTTrain(Dataset):\n",
    "#since this is a pytorch dataset we definitely need init, len, and getitem\n",
    "    def __init__(self, images, labels, indices):\n",
    "        self.images = images # the pixel data (e.g., from your train_df).\n",
    "        self.labels = labels # the ground truth labels (0–9).\n",
    "        self.indices = indices # which rows of the dataframe to use\n",
    "        \n",
    "        \n",
    "        #define the transformations we want to apply to the images\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(), #\tConvert tensor into a PIL Image so we can apply image transformations like rotation.\n",
    "            # transforms.RandomRotation(15), #Randomly rotate the image by up to ±15 degrees \n",
    "            transforms.ToTensor(), # \tConvert back from PIL Image to a PyTorch Tensor\n",
    "            transforms.Normalize([0.5], [0.5]) #Normalize the image to have zero mean and unit variance\n",
    "        ])\n",
    "        #we apply random rotation to make the model more robust to rotation, and normalize the image to have zero mean and unit variance so that model converges faster\n",
    "        # All these transformations are automatically applied every time you load a sample from the dataset!\n",
    "        \n",
    "        #IMPORTANT: only apply transforms to the training set, not the validation or test set\n",
    "        \n",
    "    #when we call len it returns the number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    #when we call getitem it returns the sample at index idx\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape((28,28)).astype(np.uint8) #we reshape the image to 28x28 (MNIST images are 28x28) and convert to uint8 to be able to apply the transformations\n",
    "        label = self.labels[idx]\n",
    "        index = self.indices[idx]\n",
    "        \n",
    "        image = self.transforms(image) #we need to apply the transformations to the image\n",
    "\n",
    "        return {'image': image, 'label': label, 'index': index} #return a dictionary with the image, label, and index -> this is the train dataset object\n",
    "    \n",
    "    \n",
    "#---- VALIDATION DATASET ---- \n",
    "\n",
    "class MNISTVal(Dataset):\n",
    "    def __init__(self, images, labels, indices):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.5], [0.5]) \n",
    "        ])\n",
    "    \n",
    "    #no transformations needed for validation set because we want the pure results of the dataset with augmentation -> however we can apply normalisation because at inference time, applying normalisation is fine. But applying augementation may harm results. \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape((28,28)).astype(np.uint8)\n",
    "        label = self.labels[idx]\n",
    "        index = self.indices[idx]\n",
    "        image = self.transforms(image) \n",
    "        \n",
    "        return {'image': image, 'label': label, 'index': index}\n",
    "        \n",
    "\n",
    "class MNISTTEst(Dataset):\n",
    "    def __init__(self, images, indices):\n",
    "        self.images = images\n",
    "        self.indices = indices\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.5], [0.5]) \n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape((28,28)).astype(np.uint8)\n",
    "        index = self.indices[idx]\n",
    "        image = self.transforms(image) \n",
    "        \n",
    "        return {'image': image, 'index': index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000\n",
      "{'image': tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5922,\n",
      "           0.9608, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,\n",
      "           0.9922, -0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,\n",
      "           0.9922, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,\n",
      "           1.0000, -0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.0039,\n",
      "           0.9922, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3961,\n",
      "           0.9922, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3961,\n",
      "           0.9922,  0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3961,\n",
      "           0.9922,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1216,\n",
      "           0.9922,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2157,\n",
      "           0.9922,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,\n",
      "           0.9922,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,\n",
      "           0.9922, -0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7020,\n",
      "           0.9373, -0.2078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4824,\n",
      "           0.9765,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5373,\n",
      "           0.9686,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           0.7725,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           0.1294,  0.9686, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           0.1294,  0.9922, -0.3569, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           0.1294,  0.9922, -0.3569, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -0.5765,  0.5216, -0.6000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]), 'label': 1, 'index': 50404}\n",
      "------------------------------\n",
      "6000\n",
      "{'image': tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -0.4667,  0.9216,  0.7804,\n",
      "          -0.6706, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0039,  0.9294,  0.9765,\n",
      "           0.7725,  0.0118, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0196,  0.9216,\n",
      "           0.9765,  0.9843,  0.5843, -0.4196, -0.8275, -0.8275, -0.9686,\n",
      "          -0.8275, -0.8275, -0.8980, -0.8275, -0.8275, -0.8275, -0.8275,\n",
      "          -0.9451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5608,\n",
      "           0.3490,  0.9843,  0.9765,  0.9765,  0.9765,  0.9765,  0.4353,\n",
      "           0.9765,  0.9765,  0.7020,  0.9765,  0.9843,  0.9765,  0.9765,\n",
      "           0.2078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000,  0.2941,  0.9765,  0.9765,  0.9765,  0.9765,  0.9843,\n",
      "           0.9765,  0.9765,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,\n",
      "           0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.2314, -0.1686, -0.1686,  0.3176,\n",
      "           0.6549,  0.6549,  0.4510, -0.1686,  0.3725,  0.9843,  0.9843,\n",
      "          -0.3412, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.0353,  0.9765,  0.5765,\n",
      "          -0.8353, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000,  0.9216,  0.9765, -0.0118,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.7882,  0.9843,  0.9765, -0.5608,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000,  0.5216,  0.9843,  0.4275, -0.9686,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -0.4980,  0.9843,  1.0000, -0.1608, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000,  0.1216,  0.9765,  0.8745, -0.6706, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.4275,  0.9294,  0.9765, -0.5137, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.0039,  0.9765,  0.5608, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -0.7176,  0.7490,  0.9765, -0.6706, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           0.4980,  0.9843,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8824,\n",
      "           0.6549,  0.9765,  0.1373, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.2471,\n",
      "           0.9765,  0.9765,  0.4824, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7098,\n",
      "           0.9765,  0.9765,  0.2706, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2941,\n",
      "           0.9765, -0.1922, -0.8902, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]), 'label': 7, 'index': 12628}\n",
      "------------------------------\n",
      "10000\n",
      "{'image': tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3412,\n",
      "           0.4510,  0.2471,  0.1843, -0.5294, -0.7176, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7412,\n",
      "           0.9922,  0.9922,  0.9922,  0.9922,  0.8902,  0.5529,  0.5529,\n",
      "           0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.5529,  0.3333,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n",
      "          -0.1059, -0.4353, -0.1059,  0.2784,  0.7804,  0.9922,  0.7647,\n",
      "           0.9922,  0.9922,  0.9922,  0.9608,  0.7961,  0.9922,  0.9922,\n",
      "           0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.8667, -0.4824, -0.8902,\n",
      "          -0.4745, -0.4745, -0.4745, -0.5373, -0.8353,  0.8510,  0.9922,\n",
      "          -0.1686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.3490,  0.9843,  0.6392,\n",
      "          -0.8588, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.8275,  0.8275,  1.0000, -0.3490,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000,  0.0118,  0.9922,  0.8667, -0.6549,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -0.5373,  0.9529,  0.9922, -0.5137, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000,  0.0431,  0.9922,  0.4667, -0.9608, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.9294,  0.6078,  0.9451, -0.5451, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -0.0118,  0.9922,  0.4275, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -0.4118,  0.9686,  0.8824, -0.5529, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510,\n",
      "           0.7333,  0.9922,  0.3020, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.5922,\n",
      "           0.9922,  0.7176, -0.7255, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7020,  0.9922,\n",
      "           0.9922, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -0.7569,  0.7569,  0.9922,\n",
      "          -0.0980, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000,  0.0431,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.5216,  0.8980,  0.9922,  0.9922,\n",
      "          -0.5922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.9922,  0.7176,\n",
      "          -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -0.0510,  0.9922,  0.6235, -0.8588,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]), 'index': 0}\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets test our datasets (just like we tested our models)\n",
    "\n",
    "#plot samples from dataset\n",
    "plt.figure() #creates new empty figure for our plots \n",
    "f, axarr = plt.subplots(1, 3) #creates a 1x3 grid of subplots, f is the figure and axarr is an array of axes (subplots) -> one subplot for each train, test, val set. \n",
    "\n",
    "train_dataset = MNISTTrain(\n",
    "    images=train_df.iloc[:, :-1].values.astype(np.uint8), #we drop the last column (labels) and convert to uint8\n",
    "    labels=train_df['label'].values,\n",
    "    indices=train_df.index.values\n",
    ")\n",
    "\n",
    "#check if len works \n",
    "print(len(train_dataset))\n",
    "\n",
    "#check if getitem works \n",
    "print(train_dataset[0])\n",
    "\n",
    "#show image of data-set \n",
    "axarr[0].imshow(train_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "#note, we know we can access via ['image'] cos we defined it in the dataset class above here: return {'image': image, 'label': label, 'index': index}\n",
    "\n",
    "axarr[0].set_title(f\"Train Image\")\n",
    "\n",
    "print(\"-\"*30)\n",
    "\n",
    "\n",
    "val_dataset = MNISTVal(\n",
    "    images=val_df.iloc[:, :-1].values.astype(np.uint8), \n",
    "    labels=val_df['label'].values,\n",
    "    indices=val_df.index.values\n",
    ")\n",
    "\n",
    "print(len(val_dataset))\n",
    "print(val_dataset[0])\n",
    "axarr[1].imshow(val_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "axarr[1].set_title(f\"Val Image\")\n",
    "print(\"-\"*30)\n",
    "\n",
    "\n",
    "#removed the labels from the test set\n",
    "test_dataset = MNISTTEst(\n",
    "    images=test_df.iloc[:, :-1].values.astype(np.uint8), \n",
    "    indices=test_df.index.values\n",
    ")\n",
    "\n",
    "\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0])\n",
    "\n",
    "axarr[2].imshow(test_dataset[0]['image'].squeeze(), cmap='gray')\n",
    "axarr[2].set_title(f\"Test Image\")\n",
    "\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our Dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:21<00:00,  1.31it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.35it/s]\n",
      " 17%|█▋        | 1/6 [01:24<07:00, 84.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 1: 2.3067\n",
      "Val loss EPOCH 1: 2.2523\n",
      "Train acc EPOCH 1: 0.1363\n",
      "Val acc EPOCH 1: 0.1950\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:15<00:00,  1.40it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.27it/s]\n",
      " 33%|███▎      | 2/6 [02:42<05:23, 80.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 2: 1.6045\n",
      "Val loss EPOCH 2: 0.8222\n",
      "Train acc EPOCH 2: 0.5055\n",
      "Val acc EPOCH 2: 0.7853\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:22<00:00,  1.29it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.03it/s]\n",
      " 50%|█████     | 3/6 [04:09<04:10, 83.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 3: 0.6382\n",
      "Val loss EPOCH 3: 0.4807\n",
      "Train acc EPOCH 3: 0.8353\n",
      "Val acc EPOCH 3: 0.8743\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:33<00:00,  1.13it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.51it/s]\n",
      " 67%|██████▋   | 4/6 [05:46<02:57, 88.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 4: 0.4413\n",
      "Val loss EPOCH 4: 0.3833\n",
      "Train acc EPOCH 4: 0.8824\n",
      "Val acc EPOCH 4: 0.8957\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [01:36<00:00,  1.10it/s]\n",
      "100%|██████████| 12/12 [00:03<00:00,  3.23it/s]\n",
      " 83%|████████▎ | 5/6 [07:27<01:33, 93.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 5: 0.3582\n",
      "Val loss EPOCH 5: 0.3005\n",
      "Train acc EPOCH 5: 0.9010\n",
      "Val acc EPOCH 5: 0.9178\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [02:16<00:00,  1.28s/it]\n",
      "100%|██████████| 12/12 [00:09<00:00,  1.30it/s]\n",
      "100%|██████████| 6/6 [09:53<00:00, 98.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Train loss EPOCH 6: 0.3036\n",
      "Val loss EPOCH 6: 0.2521\n",
      "Train acc EPOCH 6: 0.9152\n",
      "Val acc EPOCH 6: 0.9287\n",
      "------------------------------\n",
      "Time taken: 593.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we get into training loop \n",
    "\n",
    "#define loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=ADAM_WEIGHT_DECAY, betas=ADAM_BETAS)\n",
    "\n",
    "#lets estimate the time it will take to train the model \n",
    "start = timeit.default_timer()\n",
    "\n",
    "\n",
    "#we add 2 more parameters to the tqdm function: position and leave. \n",
    "for epoch in tqdm(range(EPOCHS), position =0, leave=True):\n",
    "    model.train() #take the model into train mode \n",
    "    \n",
    "    #we need to store train labels and predictions for a given epoch so we can calculate the loss and accuracy for that epoch so we initialise empty lists \n",
    "    train_labels = [] \n",
    "    train_preds = []\n",
    "    train_running_loss = 0.0 #store runnign losss which will start at 0\n",
    "     \n",
    "    #iterate through data loader \n",
    "    for idx, img_label in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
    "        #remember we return 'image' from dataset class and its a dataframe which is accessible like a dictionary\n",
    "        img = img_label['image'].float().to(device)\n",
    "        label = img_label['label'].type(torch.uint8).to(device) # remember MNIST labels are always integers between 0–9 (digits) so we store as int\n",
    "        y_pred = model(img)\n",
    "        \n",
    "        #recall the output shape is (batch_size, num_classes) aka ([512, 10]), which is a probability distribution over the 10 classes -> so we need to get the index of the highest probability\n",
    "        \n",
    "        \n",
    "        #to predict the label: \n",
    "        y_pred_label = torch.argmax(y_pred, dim=1) #take the column with the highest probability and select it -> dim 1 instead of 0 because we look across the rows of [512, 10] which is the classes\n",
    "        \n",
    "        #add to train labels. store to cpu when we can to avoid using up GPU memory \n",
    "        train_labels.extend(label.cpu().detach())\n",
    "        train_preds.extend(y_pred_label.cpu().detach())\n",
    "        \n",
    "        #3 lines below ensure that training is happening \n",
    "        \n",
    "        loss = criterion(y_pred, label) #calculate the loss -> returns a scalar tensor\n",
    "        optimizer.zero_grad() #zero the parameter gradients\n",
    "        loss.backward() #backpropagate the loss\n",
    "        optimizer.step() #update the model parameters\n",
    "        \n",
    "        #update the loss \n",
    "        train_running_loss += loss.item() #stores the loss for all batches in the epoch\n",
    "    \n",
    "    #now we update the train loss for the whole epoch     \n",
    "    train_loss = train_running_loss / len(train_dataloader) #stores the loss for 1 epoch -> average the loss over the number of batches in the epoch. so we have average loss per batch for 1 epoch. len(train_dataloader) = number of batches in the epoch\n",
    "    \n",
    "    \n",
    "    #calculate train accuracy (num of correct predictions / total num of predictions)\n",
    "    train_acc = sum(x == y for x, y in zip(train_labels, train_preds)) / len(train_labels)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    #take model into evaluation mode \n",
    "    model.eval()\n",
    "    val_labels = [] #store validation labels for a given epoch \n",
    "    val_preds = [] #store validation predictions for a given epoch \n",
    "    val_running_loss = 0.0 #store running loss for a given epoch \n",
    "    \n",
    "    #no need to track gradients in validation loop aka ensures no learning is happening in validation loop -> since we just want validation score from trained model \n",
    "    with torch.no_grad():\n",
    "        for idx, img_label in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n",
    "            img = img_label['image'].float().to(device)\n",
    "            label = img_label['label'].type(torch.uint8).to(device)\n",
    "            y_pred = model(img)\n",
    "            \n",
    "            y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "            \n",
    "            val_labels.extend(label.cpu().detach())\n",
    "            val_preds.extend(y_pred_label.cpu().detach())\n",
    "            \n",
    "            loss = criterion(y_pred, label)\n",
    "            val_running_loss += loss.item()\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = sum(x == y for x, y in zip(val_labels, val_preds)) / len(val_labels)\n",
    "\n",
    "            \n",
    "     #print as train happens \n",
    "    print(\"-\"*30)\n",
    "    print(f\"Train loss EPOCH {epoch+1}: {train_loss:.4f}\")       \n",
    "    print(f\"Val loss EPOCH {epoch+1}: {val_loss:.4f}\")\n",
    "    print(f\"Train acc EPOCH {epoch+1}: {train_acc:.4f}\")\n",
    "    print(f\"Val acc EPOCH {epoch+1}: {val_acc:.4f}\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "stop = timeit.default_timer() \n",
    "print(f\"Time taken: {stop - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer loop: \n",
    "- we loop over epochs\n",
    "\n",
    "Inner loop: \n",
    "- we loop over each batch from the dataloader \n",
    "\n",
    "\n",
    "You will see: e.g. 46/106 -> 106 is the number of batches. number of batches = dataset-size/batch-size = (54K) /512 =106 (round up) for train, 6k/512 = 12 (round up) for test. We always round up, last batch may be smaller but we still train on those samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:06<00:00,  3.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#continue with prediction on test set \n",
    "\n",
    "labels_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, sample in enumerate(tqdm(test_dataloader, position=0, leave=True)):\n",
    "        img = sample['image'].float().to(device)\n",
    "\n",
    "        y_pred = model(img)\n",
    "        y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "        labels_pred.extend([int(i) for i in y_pred_label.cpu().detach()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot test data and see our predictions \n",
    "plt.figure()\n",
    "f, axarr = plt.subplots(2, 3)\n",
    "counter = 0\n",
    "\n",
    "for i in range(2): #iterate through rows in plot \n",
    "    for j in range(3): #iterate through columns in plot \n",
    "        axarr[i, j].imshow(img[counter].squeeze(), cmap='gray')\n",
    "        axarr[i, j].set_title(f\"Predicted: {labels_pred[counter]}\")\n",
    "        counter += 1\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
