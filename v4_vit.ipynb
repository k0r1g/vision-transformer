{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Added Params (for training and testing)\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 256 \n",
    "EPOCHS = 40 ##why this high number? usually for transformers you do 1,2,3. \n",
    "\n",
    "\n",
    "#Image params \n",
    "PATCH_SIZE = 4 #we chose 4-> pixel length of 1 dimension\n",
    "IMAGE_SIZE = 56 #The MNIST dataset images are 28 × 28 pixels in size. (H,W) = (28, 28) \n",
    "IN_CHANNELS = 1 #MNIST only has 1 channel (Grayscale). Note: RGB would be 3 channels. \n",
    "NUM_CLASSES = 10 #because MNIST\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2 # 49\n",
    "\n",
    "\n",
    "# Training Params \n",
    "LEARNING_RATE = 1e-4\n",
    "DROPOUT = 0.001 \n",
    "ADAM_WEIGHT_DECAY = 0 # paper uses 0.1, set it to 0 (defautl value)\n",
    "ADAM_BETAS = (0.9, 0.999) # again from paper. \n",
    "ACTIVATION = \"gelu\" #again use the same as the paper \n",
    "\n",
    "#Encoder-Decoder Params \n",
    "NUM_HEADS = 4  \n",
    "NUM_ENCODER = 3 \n",
    "                    \n",
    "EMBED_DIM = 64 \n",
    "HIDDEN_DIM = 128 #hidden dimentsion of MLP head for classification \n",
    "\n",
    "\n",
    "\n",
    "#Set out random intiialisation seeds\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        #function that divides images into patches\n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) #equivalent to nn.Flatten(start_dim=2, end_dim=-1) -> not a learnable layer (converts patched into sequence of vectors)\n",
    "        \n",
    "            #OUTPUT SHAPE: (batch_size, embedding_dim, num_patches) AKA the full sequence of patches\n",
    "            \n",
    "        \n",
    "        #---- CLS Token ---- \n",
    "     \n",
    "        #here we define the [CLS] token. nn.Parameter is a learnable tensor (its a single parameter not a full layer)\n",
    "        # Create a random tensor of shape (1, in_channels, embedding_dim), wrap it as a learnable parameter, and assign it as the CLS token\n",
    "        self.cls_token = nn.Parameter(torch.randn(size=(1,in_channels,embedding_dim)), requires_grad=True)\n",
    "        \n",
    "        \n",
    "        #---- Positional Embedding ---- \n",
    "        \n",
    "        \n",
    "        #positional embedding is a learnable parameter \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches+1,embedding_dim)), requires_grad=True) #we add 1 to num_patches because we have the [CLS] token\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    #after patching and flattening we have a tesnor of shape (batch_size, embedding_dim, num_patches) e.g., (32, 16, 49)\n",
    "    # x = x.permute(0, 2, 1) rearranges to (batch_size, num_patches, embedding_dim) e.g., (32, 49, 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "        #here we expand the cls token so its not just the shape for 1 sample but for a batch of images\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) #expand the cls token to the batch size. x.shape[0] is the batch size. -1, -1 tells expand function to keep original dimensions. \n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        \n",
    "        #1 axis for batches, 1 axis for sequence of patches, 1 axis for embedding dimension \n",
    "        x = torch.cat([cls_token, x], dim=1) #so we want to add the CLS token to the left of the patches\n",
    "        \n",
    "        #then we need to add the position tokens to each patch \n",
    "        x = self.position_embedding + x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# #always test model after you define it    \n",
    "# model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "# x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "# print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask=None):                                 # x: (B, S, D)\n",
    "        # q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        q = self.q_proj(q_in) # (B, Seq_q, d_h)\n",
    "        k = self.k_proj(k_in) # (B, Seq_k, d_h)\n",
    "        v = self.v_proj(v_in) # (B, Seq_v, d_h)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "            \n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.drop(attn)            # (B,S,S)\n",
    "        context = attn @ v \n",
    "        return context                                 # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):          \n",
    "        \"\"\"\n",
    "        q : (B, Seq_q, D)\n",
    "        k : (B, Seq_k, D)   defaults to q if None\n",
    "        v : (B, Seq_k, D)   defaults to k if None\n",
    "        mask : (1, 1, Seq_q, Seq_k) or None\n",
    "        \"\"\"\n",
    "        k = q if k is None else k\n",
    "        v = k if v is None else v\n",
    "                                       # x: (B,S,D)\n",
    "      # run every head, collect their (B, Seq_q, d_h) outputs\n",
    "        head_outputs = [\n",
    "            head(q, k, v, mask) for head in self.heads\n",
    "        ]                                 # list length H\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, Seq_q, D)\n",
    "        return self.drop(self.out_proj(concat))\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n",
    "\n",
    "# === 6. ViT classifier head (uses Encoder) ====================================\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_patches: int = NUM_PATCHES,\n",
    "                 num_classes: int = NUM_CLASSES,\n",
    "                 patch_size: int = PATCH_SIZE,\n",
    "                 embed_dim: int = EMBED_DIM,\n",
    "                 depth: int = NUM_ENCODER,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 in_channels: int = IN_CHANNELS):\n",
    "        super().__init__()\n",
    "        self.embed = PatchEmbedding(embed_dim, patch_size,\n",
    "                                    num_patches, dropout, in_channels)\n",
    "        self.encoder = Encoder(depth, embed_dim, num_heads, dropout)\n",
    "        self.cls_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                                 # x: (B,C,H,W)\n",
    "        x = self.embed(x)                                 # (B,S,D)\n",
    "        x = self.encoder(x)                               # (B,S,D)\n",
    "        cls = x[:, 0]                                     # (B,D)\n",
    "        return self.cls_head(cls)                         # (B,num_classes)\n",
    "\n",
    "\n",
    "model = ViT(NUM_PATCHES, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODER, NUM_HEADS, DROPOUT, IN_CHANNELS).to(device)  \n",
    "x = torch.randn(512, 1, 56, 56) #dummy image\n",
    "print(model(x).shape) #expect [512, 10] -> batch of 512 (512 images) for 10 classes -> returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Synthetic 2×2–grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56×56   PatchSize: 4  → 14×14 = 196 patch tokens\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "SEQ_MAX_LEN  = 18                    #  <start> + up-to-16 digits + <finish>\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal mask \n",
    "def causal_mask(seq_len: int, device=None):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of shape (1, 1, seq_len, seq_len)\n",
    "    where mask[..., i, j] is True  if  j ≤ i   (allowed)\n",
    "                           and False if  j > i   (forbidden)\n",
    "    The leading (1,1) dimensions let it broadcast over batch and head.\n",
    "    \"\"\"\n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    mask = idx[:, None] >= idx[None, :]   # (seq_len, seq_len) True/False\n",
    "    # add dummy batch and head dimensions\n",
    "    return mask[None, None, :, :]         # (1, 1, S, S)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * 4, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \"\"\"\n",
    "        x       : (B, T, D)   decoder input so far\n",
    "        enc_out : (B, S, D)   encoder memory\n",
    "        mask    : (1, 1, T, T) causal mask\n",
    "        \"\"\"\n",
    "        # 1. masked self-attention\n",
    "        qkv_in = self.ln1(x)\n",
    "        x = x + self.self_attn(qkv_in, qkv_in, qkv_in, mask)\n",
    "\n",
    "        # 2. encoder–decoder cross-attention\n",
    "        q = self.ln2(x)\n",
    "        k = v = enc_out                        # same tensor for key and value\n",
    "        x = x + self.cross_attn(q, k, v)       # no mask here\n",
    "\n",
    "        # 3. feed-forward\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 depth: int = 3, #number of decoder blocks stacked sequentially\n",
    "                 vocab_size: int = len(VOCAB)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size) \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, SEQ_MAX_LEN, hidden_size))\n",
    "        self.start_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(hidden_size, num_heads, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, enc_out):\n",
    "        \"\"\"\n",
    "        targets : (B, 5) integer ids excluding <start>\n",
    "                  column layout: [d1 d2 d3 d4 <pad>]\n",
    "        enc_out : (B, S, D)\n",
    "        returns\n",
    "        logits  : (B, 6, vocab)\n",
    "        \"\"\"\n",
    "        B = targets.size(0) #extract batch size from input \n",
    "    \n",
    "        #prepend the learnable <start> token \n",
    "        start_vec = self.start_token.expand(B, -1, -1) #(B, 1, D)\n",
    "        tgt_vecs = self.token_embed(targets) #(B, 5, D)\n",
    "        x = torch.cat([start_vec, tgt_vecs], dim=1) #(B, 6, D)\n",
    "    \n",
    "        # add positional embeddings (same for every batch items)\n",
    "        x = x + self.pos_embed\n",
    "    \n",
    "        T = x.size(1) #sequence length including <start> token\n",
    "        \n",
    "        mask = causal_mask(T, x.device) #(1, 1, 6, 6)\n",
    "    \n",
    "        #run through blocks \n",
    "        for blk in self.blocks: \n",
    "            x = blk(x, enc_out, mask) #note the recursion here \n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTranscriber(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "        #reuse PatchEmbedding and Encoder \n",
    "        self.embed = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "        self.encoder = Encoder(NUM_ENCODER, EMBED_DIM, NUM_HEADS, DROPOUT)\n",
    "        self.decoder = DigitDecoder(EMBED_DIM, NUM_HEADS, depth = 3)\n",
    "        \n",
    "    def forward(self, images, targets): \n",
    "        enc_seq = self.encoder(self.embed(images))\n",
    "        logits = self.decoder(targets, enc_seq)\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "dummy_img = torch.randn(B, 1, 56, 56)          # batch of fake canvases\n",
    "dummy_tgt = torch.randint(0, 10, (B, 5))       # random digits 0-9\n",
    "model = GridTranscriber()\n",
    "out = model(dummy_img, dummy_tgt)\n",
    "print(\"logits shape :\", out.shape)             # should be (2, 6, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GRID_SIZE    = 4                    # 4×4 cells\n",
    "CELL_PIX     = 28 \n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX \n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # uint8 → 0-1\n",
    "    transforms.Normalize([0.5], [0.5])      # centre to −1..1\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels,\n",
    "                 epoch_size=60_000, rng=None):\n",
    "        self.base_images = base_images          # (N,28,28) uint8\n",
    "        self.base_labels = base_labels          # (N,)\n",
    "        self.epoch_size  = epoch_size\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "\n",
    "        # indices per digit for fast balanced sampling\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "        # 15 non-empty cell-occupancy patterns (bitmask 1..15)\n",
    "        patterns = np.arange(1, 1 << 16, dtype=np.uint16)\n",
    "        reps = math.ceil(epoch_size / len(patterns))\n",
    "        self.pattern_pool = self.rng.permutation(\n",
    "            np.tile(patterns, reps)[:epoch_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def _cells_from_mask(mask: int):\n",
    "        return [i for i in range(16) if (mask >> i) & 1]   # TL,TR,BL,BR\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mask     = int(self.pattern_pool[idx])\n",
    "        cell_ids = self._cells_from_mask(mask)\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        odd_list, even_list = [], []\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))\n",
    "            img_idx = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "\n",
    "            row, col = divmod(cell, GRID_SIZE)\n",
    "            top, left = row*CELL_PIX, col*CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "\n",
    "            # bucket into odd/even lists\n",
    "            (odd_list if d % 2 else even_list).append(d)\n",
    "\n",
    "        #Build the target sequence: \n",
    "        odd_list.sort()\n",
    "        even_list.sort(reverse=True)\n",
    "        seq = [VOCAB['<start>']] + [VOCAB[str(d)] for d in odd_list] + [VOCAB[str(d)] for d in even_list] + [VOCAB['<finish>']]\n",
    "        \n",
    "        #pad to fixed length \n",
    "        length = len(seq)\n",
    "        seq += [PAD_IDX] * (SEQ_MAX_LEN - length)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'image'  : grid_transforms(canvas),          # (1,56,56) float\n",
    "            'target' : torch.tensor(seq, dtype=torch.long),  # (6,)\n",
    "            'length' : length\n",
    "        }\n",
    "\n",
    "# ─── build train / val / test loaders ───────────────────────────────────────\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),   # we reuse MNIST test set\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# Decide how many CPU cores to devote to data loading.\n",
    "# Four to eight usually keeps the GPU fed without wasting resources.\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "NUM_WORKERS = min(8, os.cpu_count())        # 4-8 is a good starting range\n",
    "\n",
    "loader_kwargs = dict(\n",
    "    batch_size      = BATCH_SIZE,\n",
    "    num_workers     = NUM_WORKERS,          # <- key addition\n",
    "    pin_memory      = True,                 # speeds up host-to-device copy\n",
    "    persistent_workers = True,              # keeps workers alive across epochs\n",
    "    prefetch_factor = 4                     # each worker holds 4 batches ready\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_grid,\n",
    "    shuffle = True,\n",
    "    **loader_kwargs                           # unpack the common arguments\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_grid,\n",
    "    shuffle = False,\n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_grid,\n",
    "    shuffle = False,\n",
    "    **loader_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little smoke test:\n",
    "sample = GridMNIST(train_dataset.data.numpy(),\n",
    "                   train_dataset.targets.numpy(),\n",
    "                   epoch_size=1, rng=0)[0]\n",
    "plt.imshow(sample['image'].squeeze(), cmap='gray'); plt.axis('off')\n",
    "print(\"target ids :\", sample['target'].tolist())\n",
    "print(\"target txt :\", ' '.join(VOCAB_INV[i] for i in sample['target']\n",
    "                               if i != PAD_IDX))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt  = ' '.join([str(VOCAB_INV[t]) for t in target if t != PAD_IDX])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  1. helper: exact-match metric\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def sequence_exact_match(logits, targets):\n",
    "    \"\"\"\n",
    "    logits  : (B, 6, vocab)  – raw scores from the model\n",
    "    targets : (B, 6)         – ground-truth ids  (<start> … <finish> <pad>)\n",
    "    Returns a float in [0,1] = proportion of sequences that match exactly\n",
    "    (ignoring <pad>).\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(dim=-1)                     # (B,6) choose top id\n",
    "    # a token counts as correct if ids equal OR gold token is PAD\n",
    "    match = (preds == targets) | (targets == PAD_IDX) # bool (B,6)\n",
    "    seq_match = match.all(dim=1)                      # bool (B,)\n",
    "    return seq_match.float().mean().item()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  2. model, loss and optimiser\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "model = GridTranscriber().to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                        lr=LEARNING_RATE,\n",
    "                        weight_decay=ADAM_WEIGHT_DECAY,\n",
    "                        betas=ADAM_BETAS)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  3. training / validation loop\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ── TRAIN ────────────────────────────────────────────────────────────\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_em   = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"train {epoch+1}/{EPOCHS}\"):\n",
    "        imgs    = batch['image'].to(device)    # (B,1,56,56)\n",
    "        targets = batch['target'].to(device)   # (B,6)\n",
    "\n",
    "        decoder_in  = targets[:, 1:]           # drop <start>  → (B,5)\n",
    "        gold_tokens = targets                  # keep full seq (B,6)\n",
    "\n",
    "        logits = model(imgs, decoder_in)       # (B,6,vocab)\n",
    "\n",
    "        # loss = criterion(logits.view(-1, len(VOCAB)),\n",
    "        #                  gold_tokens.view(-1))\n",
    "        loss = criterion(logits.reshape(-1, len(VOCAB)),\n",
    "                         gold_tokens.reshape(-1))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_em   += sequence_exact_match(logits, gold_tokens)\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_em   /= len(train_dataloader)\n",
    "\n",
    "    # ── VALIDATE ─────────────────────────────────────────────────────────\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_em   = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"valid\"):\n",
    "            imgs    = batch['image'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "\n",
    "            decoder_in  = targets[:, 1:]\n",
    "            gold_tokens = targets\n",
    "\n",
    "            logits = model(imgs, decoder_in)\n",
    "            # loss   = criterion(logits.view(-1, len(VOCAB)),\n",
    "            #                    gold_tokens.view(-1))\n",
    "            \n",
    "            loss = criterion(logits.reshape(-1, len(VOCAB)),\n",
    "                         gold_tokens.reshape(-1))\n",
    "            \n",
    "            \n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_em   += sequence_exact_match(logits, gold_tokens)\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_em   /= len(val_dataloader)\n",
    "\n",
    "    print(\"─\" * 60)\n",
    "    print(f\"Epoch {epoch+1:02d}/{EPOCHS}\")\n",
    "    print(f\"  train loss {train_loss:.4f}   EM {train_em:.4f}\")\n",
    "    print(f\"  val   loss {val_loss:.4f}   EM {val_em:.4f}\")\n",
    "\n",
    "stop_time = timeit.default_timer()\n",
    "print(f\"Total training time: {stop_time - start_time:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to see our training loop \n",
    "sample = next(iter(val_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()\n",
    "gold_seq = sample['target'][0].tolist()\n",
    "gold_txt = ' '.join([IDX_TO_SYM[t] for t in gold_seq if t != PAD_IDX])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(sample['image'].to(device),\n",
    "                   sample['target'][:,1:].to(device))\n",
    "pred_seq = logits.argmax(dim=-1)[0].cpu().tolist()\n",
    "pred_txt = ' '.join([IDX_TO_SYM[t] for t in pred_seq if t != PAD_IDX])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray'); plt.axis('off')\n",
    "plt.title(f\"gold: {gold_txt}     pred: {pred_txt}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to review this. \n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  AUTOREGRESSIVE INFERENCE ON THE TEST SET\n",
    "#  • Greedy decoding (arg-max at each step)\n",
    "#  • Stops when <finish> is produced or when 5 digits are generated\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def greedy_decode(model, images):\n",
    "    \"\"\"\n",
    "    images : (B, 1, 56, 56)  torch.float32\n",
    "    returns\n",
    "    pred_seq : (B, 6)  int, includes <start>  …  <finish>  <pad> ...\n",
    "    \"\"\"\n",
    "    B = images.size(0)\n",
    "    device = images.device\n",
    "\n",
    "    # encode once\n",
    "    enc_seq = model.encoder(model.embed(images))          # (B,S,D)\n",
    "\n",
    "    # first input to decoder = just <start>\n",
    "    start_id = VOCAB['<start>']\n",
    "    decoder_input = torch.full((B, 1), start_id,\n",
    "                               dtype=torch.long, device=device)  # (B,1)\n",
    "\n",
    "    finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "    outputs  = [decoder_input.squeeze(1)]                  # list of (B,)\n",
    "\n",
    "    for step in range(5):                                  # at most 5 more\n",
    "        logits = model.decoder(decoder_input, enc_seq)     # (B, step+1, V)\n",
    "        next_token = logits[:, -1].argmax(dim=-1)          # (B,)\n",
    "\n",
    "        outputs.append(next_token)\n",
    "\n",
    "        finished |= (next_token == VOCAB['<finish>'])\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "        decoder_input = torch.cat([decoder_input,\n",
    "                                   next_token.unsqueeze(1)], dim=1)  # append\n",
    "\n",
    "    # pad to length 6\n",
    "    while len(outputs) < 6:\n",
    "        outputs.append(torch.full((B,), PAD_IDX, device=device))\n",
    "\n",
    "    return torch.stack(outputs, dim=1)   # (B,6)\n",
    "\n",
    "# ─── evaluate on the whole test set ──────────────────────────────────────────\n",
    "model.eval()\n",
    "exact_matches = 0\n",
    "total_batches = 0\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"test\"):\n",
    "    imgs   = batch['image'].to(device)\n",
    "    gold   = batch['target'].to(device)          # (B,6)\n",
    "\n",
    "    pred = greedy_decode(model, imgs)            # (B,6)\n",
    "\n",
    "    match = ((pred == gold) | (gold == PAD_IDX)).all(dim=1)\n",
    "    exact_matches += match.sum().item()\n",
    "    total_batches += gold.size(0)\n",
    "\n",
    "test_em = exact_matches / total_batches\n",
    "print(f\"\\nExact-match accuracy on test set : {test_em:.4f}\")\n",
    "\n",
    "# ─── visualise a few samples ────────────────────────────────────────────────\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    toks = [VOCAB_INV[i] for i in seq if i not in (PAD_IDX,)]\n",
    "    # remove repeated <start> if any, keep tokens until <finish>\n",
    "    result = []\n",
    "    for t in toks:\n",
    "        result.append(t)\n",
    "        if t == '<finish>':\n",
    "            break\n",
    "    return ' '.join(result)\n",
    "\n",
    "n_rows, n_cols = 2, 4\n",
    "plt.figure(figsize=(n_cols*2.5, n_rows*2.5))\n",
    "\n",
    "sample_iter = iter(test_dataloader)\n",
    "batch = next(sample_iter)\n",
    "imgs = batch['image'].to(device)\n",
    "gold = batch['target']\n",
    "pred = greedy_decode(model, imgs).cpu()\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    ax = plt.subplot(n_rows, n_cols, i+1)\n",
    "    ax.imshow(imgs[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"G: {seq_to_text(gold[i])}\\nP: {seq_to_text(pred[i])}\",\n",
    "                 fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
