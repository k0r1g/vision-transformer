{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as TF   #  <-- add this line\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of patches: 64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Training Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 256 #change to 256 if memory cant handle 512\n",
    "LEARNING_RATE = 1e-4\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "\n",
    "# Dataset Parameters\n",
    "NUM_CLASSES = 10\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "\n",
    "# Data Grids Parameters\n",
    "GRID_SIZE    = 4                    # 4Ã—4 cells\n",
    "CELL_PIX     = 14\n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX \n",
    "IMAGE_SIZE = CANVAS_PIX\n",
    "\n",
    "# Vision Transformer (ViT) Architecture Parameters\n",
    "PATCH_SIZE = 7\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 128\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODER = 3\n",
    "\n",
    "\n",
    "# Setup & Initialization\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True # Optional settings for reproducibility\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Number of patches: {NUM_PATCHES}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "#function that divides images into patches\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) \n",
    "       \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches,embedding_dim)), requires_grad=True)  #Positional Embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        x = self.position_embedding + x #add posi\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# #always test model after you define it    \n",
    "# model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "# x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "# print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask=None):                                 # x: (B, S, D)\n",
    "        # q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        q = self.q_proj(q_in) # (B, Seq_q, d_h)\n",
    "        k = self.k_proj(k_in) # (B, Seq_k, d_h)\n",
    "        v = self.v_proj(v_in) # (B, Seq_v, d_h)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "            \n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.drop(attn)            # (B,S,S)\n",
    "        context = attn @ v \n",
    "        return context                                 # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):          \n",
    "        \"\"\"\n",
    "        q : (B, Seq_q, D)\n",
    "        k : (B, Seq_k, D)   defaults to q if None\n",
    "        v : (B, Seq_k, D)   defaults to k if None\n",
    "        mask : (1, 1, Seq_q, Seq_k) or None\n",
    "        \"\"\"\n",
    "        k = q if k is None else k\n",
    "        v = q if v is None else v\n",
    "\n",
    "        # run every head, collect their (B, Seq_q, d_h) outputs\n",
    "        head_outputs = [\n",
    "            head(q, k, v, mask) for head in self.heads\n",
    "        ]                                 # list length H\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, Seq_q, D)\n",
    "\n",
    "        out = self.out_proj(concat)  # cache the projection\n",
    "        # print(\"      [MultiHeadAttention] q:\", q.shape)\n",
    "        # print(\"      [MultiHeadAttention] k:\", k.shape)\n",
    "        # print(\"      [MultiHeadAttention] v:\", v.shape)\n",
    "        # print(\"      [MultiHeadAttention] output:\", out.shape)\n",
    "\n",
    "        return self.drop(out)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Synthetic 2Ã—2â€“grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56Ã—56   PatchSize: 4  â†’ 14Ã—14 = 196 patch tokens\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "MAX_DECODER_LEN  = 128                    #  some big number\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal mask \n",
    "def causal_mask(seq_len: int, device=None):\n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    mask = idx[:, None] >= idx[None, :]          # (T, T) bool\n",
    "    return mask[None, :, :]                      # (1, T, T)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * 4, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \"\"\"\n",
    "        x       : (B, T, D)   decoder input so far\n",
    "        enc_out : (B, S, D)   encoder memory\n",
    "        mask    : (1, 1, T, T) causal mask\n",
    "        \"\"\"\n",
    "        # 1. masked self-attention\n",
    "        qkv_in = self.ln1(x)\n",
    "        x = x + self.self_attn(qkv_in, qkv_in, qkv_in, mask)\n",
    "\n",
    "        # 2. encoderâ€“decoder cross-attention\n",
    "        q = self.ln2(x)\n",
    "        k = v = enc_out                        # same tensor for key and value\n",
    "        x = x + self.cross_attn(q, k, v)       # no mask here\n",
    "\n",
    "        # 3. feed-forward\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 depth: int = 3, #number of decoder blocks stacked sequentially\n",
    "                 vocab_size: int = len(VOCAB)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size) \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, MAX_DECODER_LEN, hidden_size))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(hidden_size, num_heads, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, enc_out):\n",
    "        \"\"\"\n",
    "        targets : (B, T) integer ids including <start> at index 0\n",
    "        enc_out : (B, S, D)\n",
    "        returns\n",
    "        logits  : (B, T-1, vocab) â€“ predictions for d1, d2, â€¦, <finish>\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = targets.size()\n",
    "        \n",
    "        \n",
    "        token_inputs = targets[:, :-1] #(B, T-1)\n",
    "        # print(\"  token_inputs:\", token_inputs.shape)\n",
    "        x = self.token_embed(token_inputs) #(B, T-1, D)\n",
    "        # print(\"  embedded x:\", x.shape)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :] \n",
    "        # print(\"    [DecoderBlock] x input :\", x.shape)\n",
    "        \n",
    "        mask = causal_mask(T-1, x.device)        # (1,1,T-1,T-1)\n",
    "\n",
    "\n",
    "        #run through blocks \n",
    "        for blk in self.blocks: \n",
    "            x = blk(x, enc_out, mask) \n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        # print(\"  logits before return:\", logits.shape)\n",
    "        return logits \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTranscriber(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "        #reuse PatchEmbedding and Encoder \n",
    "        self.embed = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "        self.encoder = Encoder(NUM_ENCODER, EMBED_DIM, NUM_HEADS, DROPOUT)\n",
    "        self.decoder = DigitDecoder(EMBED_DIM, NUM_HEADS, depth = 3)\n",
    "        \n",
    "    def forward(self, images, targets): \n",
    "        # enc_seq = self.encoder(self.embed(images))\n",
    "        # logits = self.decoder(targets, enc_seq)\n",
    "        # return logits \n",
    "        # print(\"INPUT images shape:\", images.shape)\n",
    "        x = self.embed(images)\n",
    "        # print(\"AFTER PatchEmbedding:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(\"AFTER Encoder:\", x.shape)\n",
    "        logits = self.decoder(targets, x)\n",
    "        # print(\"AFTER Decoder:\", logits.shape)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "dummy_img = torch.randn(B, 1, 56, 56)          # batch of fake canvases\n",
    "dummy_tgt = torch.randint(0, 10, (B, 5))       # random digits 0-9\n",
    "model = GridTranscriber()\n",
    "out = model(dummy_img, dummy_tgt)\n",
    "# print(\"logits shape :\", out.shape)             # should be (2, 6, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digitâ€‘count histogram (sample of 5â€¯k): Counter({11: 349, 13: 346, 15: 335, 4: 329, 5: 320, 3: 317, 12: 314, 2: 313, 8: 310, 10: 309, 1: 307, 6: 302, 9: 294, 14: 287, 16: 284, 7: 284})\n"
     ]
    }
   ],
   "source": [
    "# %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Gridâ€‘MNIST (4Ã—4, up to 16 digits) with stratified sampling by digitâ€‘count\n",
    "import os, math, numpy as np, torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "GRID_SIZE   = 4            # 4Ã—4 grid\n",
    "CELL_PIX    = 14\n",
    "CANVAS_PIX  = GRID_SIZE * CELL_PIX   # 56\n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels,\n",
    "                 epoch_size=60_000, rng=None):\n",
    "        self.base_images  = base_images          # (N,28,28) uint8\n",
    "        self.base_labels  = base_labels          # (N,)\n",
    "        self.epoch_size   = epoch_size\n",
    "        self.rng          = np.random.default_rng(rng)\n",
    "\n",
    "        # fast index by digit\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "        # â”€â”€ 1. build a balanced pool of desired digitâ€‘counts (1â€¦16) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        per_len   = self.epoch_size // 16                      # 60â€¯000 â†’ 3â€¯750 each\n",
    "        counts    = np.arange(1, 17)                           # [1,â€¦,16]\n",
    "        pool      = np.tile(counts, per_len)                   # 60â€¯000 total\n",
    "        self.count_pool = self.rng.permutation(pool)           # shuffle\n",
    "\n",
    "    # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    # choose N distinct cell IDs out of 16\n",
    "    def _random_cells(self, n):\n",
    "        return self.rng.choice(16, size=n, replace=False)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        n_digits  = int(self.count_pool[idx])      # how many digits to place\n",
    "        cell_ids  = self._random_cells(n_digits)\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        digits = []\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))                   # 0â€‘9 uniform\n",
    "            img_idx   = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "\n",
    "            digit_img = TF.resize(Image.fromarray(digit_img), CELL_PIX)\n",
    "            digit_img = np.array(digit_img, dtype=np.uint8)\n",
    "\n",
    "            row, col  = divmod(cell, GRID_SIZE)\n",
    "            top, left = row * CELL_PIX, col * CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "            digits.append(d)\n",
    "\n",
    "        # build target: odds â†‘asc  + evens â†“desc\n",
    "        odds   = sorted([d for d in digits if d % 2 == 1])\n",
    "        evens  = sorted([d for d in digits if d % 2 == 0], reverse=True)\n",
    "        seq    = [VOCAB['<start>']] + odds + evens + [VOCAB['<finish>']]\n",
    "\n",
    "        return {\n",
    "            \"image\" : grid_transforms(canvas),                 # (1,56,56)\n",
    "            \"target\": torch.tensor(seq, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# â”€â”€ collate ---------------------------------------------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_batch(batch):\n",
    "    imgs  = torch.stack([b[\"image\"]  for b in batch])\n",
    "    tgts  = [b[\"target\"] for b in batch]\n",
    "    tgts  = pad_sequence(tgts, batch_first=True, padding_value=PAD_IDX)\n",
    "    return {\"image\": imgs, \"target\": tgts}\n",
    "\n",
    "# â”€â”€ build loaders ---------------------------------------------------------\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "NUM_WORKERS = min(8, os.cpu_count())\n",
    "loader_kw   = dict(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                   pin_memory=True, persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "train_dataloader = DataLoader(train_grid, shuffle=True,  collate_fn=collate_batch, **loader_kw)\n",
    "val_dataloader   = DataLoader(val_grid,   shuffle=False, collate_fn=collate_batch, **loader_kw)\n",
    "test_dataloader  = DataLoader(test_grid,  shuffle=False, collate_fn=collate_batch, **loader_kw)\n",
    "\n",
    "# quick sanityâ€‘check: verify balanced digit counts\n",
    "if __name__ == \"__main__\":\n",
    "    from collections import Counter\n",
    "    digit_hist = Counter(len([t for t in ex[\"target\"][1:-1]]) for ex in [train_grid[i] for i in range(5000)])\n",
    "    print(\"Digitâ€‘count histogram (sample of 5â€¯k):\", digit_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save samples:\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "SAVE_DIR = \"./v4_debug_samples\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "N = 100  # how many samples to save\n",
    "\n",
    "for i in range(N):\n",
    "    sample = train_grid[i]  # uses your GridMNIST dataset\n",
    "    img = sample[\"image\"].squeeze(0)  # (56,56)\n",
    "    tgt = sample[\"target\"].tolist()\n",
    "    tokens = [VOCAB_INV[t] for t in tgt]\n",
    "\n",
    "    # Save image\n",
    "    img_pil = TF.to_pil_image(img * 0.5 + 0.5)  # unnormalize\n",
    "    img_pil.save(os.path.join(SAVE_DIR, f\"{i:03d}_{'-'.join(tokens)}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§® Digit Count Distribution:\n",
      "  1 digits: 3,750\n",
      "  2 digits: 3,750\n",
      "  3 digits: 3,750\n",
      "  4 digits: 3,750\n",
      "  5 digits: 3,750\n",
      "  6 digits: 3,750\n",
      "  7 digits: 3,750\n",
      "  8 digits: 3,750\n",
      "  9 digits: 3,750\n",
      "  10 digits: 3,750\n",
      "  11 digits: 3,750\n",
      "  12 digits: 3,750\n",
      "  13 digits: 3,750\n",
      "  14 digits: 3,750\n",
      "  15 digits: 3,750\n",
      "  16 digits: 3,750\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#see results:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "digit_counts = []\n",
    "\n",
    "for i in range(len(train_grid)):\n",
    "    target = train_grid[i]['target'].tolist()\n",
    "    tokens = [VOCAB_INV[t] for t in target]\n",
    "\n",
    "    try:\n",
    "        start = tokens.index('<start>')\n",
    "        finish = tokens.index('<finish>')\n",
    "        digits = tokens[start + 1 : finish]\n",
    "        digit_counts.append(len(digits))\n",
    "    except Exception:\n",
    "        print(\"Bad sample:\", tokens)\n",
    "\n",
    "# Count how many sequences have 1, 2, 3, 4 digits\n",
    "counts = Counter(digit_counts)\n",
    "\n",
    "# Print the exact numbers\n",
    "print(\"ðŸ§® Digit Count Distribution:\")\n",
    "for num_digits in sorted(counts.keys()):\n",
    "    print(f\"  {num_digits} digits: {counts[num_digits]:,}\")\n",
    "\n",
    "# Plot the distribution\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.xlabel(\"Number of Digits per Sequence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Digit Count Distribution in Training Set\")\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt = ' '.join([VOCAB_INV[t] for t in target   # after padding\n",
    "                    if t not in (PAD_IDX,)])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Token Distribution (Sample Batch) ---\n",
      "       0: 246\n",
      "       1: 225\n",
      "       2: 228\n",
      "       3: 234\n",
      "       4: 218\n",
      "       5: 256\n",
      "       6: 236\n",
      "       7: 214\n",
      "       8: 250\n",
      "       9: 197\n",
      " <start>: 256\n",
      "<finish>: 256\n",
      "   <pad>: 1792\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [02:55<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1 Validation Sample 0 --\n",
      "PRED (17 tokens): 1 3 5 <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish> <finish>\n",
      "GOLD (5 tokens): 1 3 2 0 <finish>\n",
      "-------------------------------------\n",
      "Epoch 1/10 â”‚ train loss 1.6829 â”‚ EM 0.0014 â”‚ tok_acc 0.3894 â”‚ val loss 1.2772 â”‚ EM 0.0047 â”‚ tok_acc 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:21<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 â”‚ train loss 1.2035 â”‚ EM 0.0063 â”‚ tok_acc 0.4913 â”‚ val loss 1.1429 â”‚ EM 0.0091 â”‚ tok_acc 0.5133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:09<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 â”‚ train loss 1.0911 â”‚ EM 0.0123 â”‚ tok_acc 0.5329 â”‚ val loss 1.0449 â”‚ EM 0.0192 â”‚ tok_acc 0.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:23<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 â”‚ train loss 1.0083 â”‚ EM 0.0251 â”‚ tok_acc 0.5683 â”‚ val loss 0.9729 â”‚ EM 0.0347 â”‚ tok_acc 0.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:09<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 â”‚ train loss 0.9612 â”‚ EM 0.0332 â”‚ tok_acc 0.5901 â”‚ val loss 0.9402 â”‚ EM 0.0499 â”‚ tok_acc 0.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:48<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 â”‚ train loss 0.9338 â”‚ EM 0.0383 â”‚ tok_acc 0.6002 â”‚ val loss 0.9109 â”‚ EM 0.0491 â”‚ tok_acc 0.6074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:28<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 â”‚ train loss 0.9125 â”‚ EM 0.0415 â”‚ tok_acc 0.6079 â”‚ val loss 0.8934 â”‚ EM 0.0533 â”‚ tok_acc 0.6181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 â”‚ train loss 0.8903 â”‚ EM 0.0462 â”‚ tok_acc 0.6173 â”‚ val loss 0.8906 â”‚ EM 0.0377 â”‚ tok_acc 0.6182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:44<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 â”‚ train loss 0.8656 â”‚ EM 0.0489 â”‚ tok_acc 0.6282 â”‚ val loss 0.8538 â”‚ EM 0.0569 â”‚ tok_acc 0.6337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [03:21<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 â”‚ train loss 0.8418 â”‚ EM 0.0545 â”‚ tok_acc 0.6395 â”‚ val loss 0.8255 â”‚ EM 0.0576 â”‚ tok_acc 0.6467\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0.  Optimizer & Loss\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "criterion = nn.CrossEntropyLoss()          # PAD will be masked manually\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    "    betas=ADAM_BETAS\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  Exactâ€‘match metric (ignores PAD)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def sequence_exact_match(pred_ids, gold_ids):\n",
    "    ok = (pred_ids == gold_ids) | (gold_ids == PAD_IDX)\n",
    "    return ok.all(dim=1).float().mean().item()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NEW: Print target token distribution from a batch\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n--- Target Token Distribution (Sample Batch) ---\")\n",
    "sample_batch = next(iter(train_dataloader))  # use one batch to estimate\n",
    "all_tokens = sample_batch[\"target\"].flatten()\n",
    "counts = torch.bincount(all_tokens, minlength=len(VOCAB))\n",
    "for i, c in enumerate(counts):\n",
    "    print(f\"{VOCAB_INV[i]:>8}: {c}\")\n",
    "print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  Train / Validation loop\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for epoch in range(EPOCHS):\n",
    "    # -------- TRAIN ------------------------------------------------------\n",
    "    model.train()\n",
    "    tr_loss = tr_em = tr_tok = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"train {epoch+1}/{EPOCHS}\"):\n",
    "        imgs    = batch[\"image\"].to(device)     # (B, 1, H, W)\n",
    "        targets = batch[\"target\"].to(device)    # (B, T)  <start>â€¦<finish>\n",
    "\n",
    "        dec_in   = targets                      # (B, T)\n",
    "        gold_out = targets[:, 1:]               # (B, Tâ€‘1)\n",
    "\n",
    "        logits   = model(imgs, dec_in)          # (B, Tâ€‘1, V)\n",
    "        assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "        mask            = gold_out != PAD_IDX\n",
    "        logits_flat     = logits.reshape(-1, len(VOCAB))\n",
    "        gold_flat       = gold_out.reshape(-1)\n",
    "\n",
    "        logits_masked   = logits_flat[mask.reshape(-1)]\n",
    "        gold_masked     = gold_flat[mask.reshape(-1)]\n",
    "\n",
    "        loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds    = logits.argmax(dim=-1)\n",
    "\n",
    "        # â”€â”€â”€ Token Accuracy (excluding PAD) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        tok_correct = (preds == gold_out) & mask\n",
    "        tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_em   += sequence_exact_match(preds, gold_out)\n",
    "        tr_tok  += tok_acc\n",
    "\n",
    "    tr_loss /= len(train_dataloader)\n",
    "    tr_em   /= len(train_dataloader)\n",
    "    tr_tok  /= len(train_dataloader)\n",
    "\n",
    "    # -------- VALIDATION -------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = val_em = val_tok = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            imgs    = batch[\"image\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            dec_in   = targets\n",
    "            gold_out = targets[:, 1:]\n",
    "\n",
    "            logits   = model(imgs, dec_in)\n",
    "            assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "            mask            = gold_out != PAD_IDX\n",
    "            logits_flat     = logits.reshape(-1, len(VOCAB))\n",
    "            gold_flat       = gold_out.reshape(-1)\n",
    "\n",
    "            logits_masked   = logits_flat[mask.reshape(-1)]\n",
    "            gold_masked     = gold_flat[mask.reshape(-1)]\n",
    "\n",
    "            loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "            preds    = logits.argmax(dim=-1)\n",
    "\n",
    "            # â”€â”€â”€ Token Accuracy (excluding PAD) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            tok_correct = (preds == gold_out) & mask\n",
    "            tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_em   += sequence_exact_match(preds, gold_out)\n",
    "            val_tok  += tok_acc\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_em   /= len(val_dataloader)\n",
    "    val_tok  /= len(val_dataloader)\n",
    "\n",
    "    # --- Sanity check: Visualize predictions vs gold for first epoch --- \n",
    "    if epoch == 0: \n",
    "        idx = 0\n",
    "        if preds.shape[0] > idx:\n",
    "            pred_seq = preds[idx].tolist()\n",
    "            gold_seq = gold_out[idx].tolist()\n",
    "            pred_str = [VOCAB_INV[t] for t in pred_seq if t != PAD_IDX]\n",
    "            gold_str = [VOCAB_INV[t] for t in gold_seq if t != PAD_IDX]\n",
    "            print(f\"-- Epoch {epoch+1} Validation Sample {idx} --\")\n",
    "            print(f\"PRED ({len(pred_str)} tokens): {' '.join(pred_str)}\")\n",
    "            print(f\"GOLD ({len(gold_str)} tokens): {' '.join(gold_str)}\")\n",
    "            print(\"-------------------------------------\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} â”‚ \"\n",
    "          f\"train loss {tr_loss:.4f} â”‚ EM {tr_em:.4f} â”‚ tok_acc {tr_tok:.4f} â”‚ \"\n",
    "          f\"val loss {val_loss:.4f} â”‚ EM {val_em:.4f} â”‚ tok_acc {val_tok:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 62\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# 5.Â Run it\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_embed_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# adjust to your taskâ€™s longest sequence\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 50\u001b[0m, in \u001b[0;36mgreedy_decode\u001b[0;34m(model, test_image, patch_embed, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     49\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(ys, mem)          \u001b[38;5;66;03m# (1, t, vocab)   t = ys_lenâ€‘1\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     next_id \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)       \u001b[38;5;66;03m# (1,)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ys \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([ys, next_id\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_id\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m finish_id:\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 1 with size 0"
     ]
    }
   ],
   "source": [
    "# %% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Inference: greedy decoding on a single test image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0.Â Move model to the device you want to run on\n",
    "device = \"cpu\"        # â† change to \"cuda\" if you want GPU decoding\n",
    "model = model.to(device).eval()      # make sure this is your trained model\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1.Â Grab a single test image  (shape: 1Ã—56Ã—56)  and move to same device\n",
    "sample      = next(iter(test_dataloader))         # batch dict\n",
    "test_image  = sample[\"image\"][0].to(device)       # (1,56,56) tensor\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Minimal tokenizer wrapper around VOCAB / VOCAB_INV dicts\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab, vocab_inv):\n",
    "        self.token_to_idx = vocab\n",
    "        self.idx_to_token = vocab_inv\n",
    "\n",
    "tokenizer = SimpleTokenizer(VOCAB, VOCAB_INV)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3.Â Point to the patchâ€‘embedding layer (lives in model.embed)\n",
    "patch_embed_layer = model.embed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  Greedyâ€‘decode (function you defined earlier)\n",
    "def preprocess(image, patch_embed):\n",
    "    image = image.unsqueeze(0)                # (1,1,H,W)\n",
    "    return patch_embed(image)                 # (1, seq_len, D)\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    return [tokenizer.idx_to_token[i] for i in token_ids]\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, test_image, patch_embed, tokenizer, max_len=10):\n",
    "    encoder_input  = preprocess(test_image, patch_embed)\n",
    "    mem = model.encoder(encoder_input)\n",
    "\n",
    "    start_id  = tokenizer.token_to_idx[\"<start>\"]\n",
    "    finish_id = tokenizer.token_to_idx[\"<finish>\"]\n",
    "\n",
    "    ys = torch.tensor([[start_id]], dtype=torch.long, device=test_image.device)  # (1,1)\n",
    "\n",
    "    for _ in range(max_len - 1):\n",
    "        logits = model.decoder(ys, mem)          # (1, t, vocab)   t = ys_lenâ€‘1\n",
    "        next_id = logits[:, -1].argmax(-1)       # (1,)\n",
    "        ys = torch.cat([ys, next_id.unsqueeze(1)], dim=1)\n",
    "\n",
    "        if next_id.item() == finish_id:\n",
    "            break\n",
    "\n",
    "    tokens = [tokenizer.idx_to_token[i] for i in ys[0].tolist()]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.Â Run it\n",
    "generated = greedy_decode(\n",
    "    model       = model,\n",
    "    test_image  = test_image,\n",
    "    patch_embed = patch_embed_layer,\n",
    "    tokenizer   = tokenizer,\n",
    "    max_len     = 8        # adjust to your taskâ€™s longest sequence\n",
    ")\n",
    "\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
