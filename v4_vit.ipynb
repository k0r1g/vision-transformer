{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import pandas as pd\n",
    "from torch import optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms \n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import random \n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as TF   #  <-- add this line\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Architecture from the 16x16 words paper\n",
    "\n",
    "![vit_architecture](ViT_architecture.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of patches: 196\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Training Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 256 #change to 256 if memory cant handle 512\n",
    "LEARNING_RATE = 1e-4\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "\n",
    "# Dataset Parameters\n",
    "NUM_CLASSES = 10\n",
    "IN_CHANNELS = 1\n",
    "\n",
    "\n",
    "# Data Grids Parameters\n",
    "GRID_SIZE    = 4                    # 4×4 cells\n",
    "CELL_PIX     = 14\n",
    "CANVAS_PIX   = GRID_SIZE * CELL_PIX \n",
    "IMAGE_SIZE = CANVAS_PIX\n",
    "\n",
    "# Vision Transformer (ViT) Architecture Parameters\n",
    "PATCH_SIZE = 4\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "EMBED_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 128\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODER = 3\n",
    "\n",
    "\n",
    "# Setup & Initialization\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True # Optional settings for reproducibility\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Number of patches: {NUM_PATCHES}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CLS Tokens and merging with Positional Embeddings \n",
    "\n",
    "#function that divides images into patches\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_size, num_patches, dropout, in_channels): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            # all Conv2d does is divide our image into patch sizes\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embedding_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            ), \n",
    "            nn.Flatten(start_dim=2)) \n",
    "       \n",
    "        self.position_embedding = nn.Parameter(torch.randn(size=(1,num_patches,embedding_dim)), requires_grad=True)  #Positional Embedding\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = self.patcher(x).permute(0,2,1) # first patch x through patcher -> where nn.Conv2d: splits x into patches and embeds them, nn.Flatten(start_dim=2) converts into 1D sequence\n",
    "        x = self.position_embedding + x #add posi\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# #always test model after you define it    \n",
    "# model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS).to(device)  \n",
    "# x = torch.randn(512, 1, 56, 56).to(device)   #create dummy image of batch size 512, channels 1, and dimensions 28x28 \n",
    "# print(model(x).shape) #expect (512, 50, 16) where batch size 512, 50 is number of tokens we feed transformer (correct because we have 49 patches + CLS token), 16 is size of patches (embedding dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# === Helper activation ========================================================\n",
    "class NewGELUActivation(nn.Module):                       # same formula as HF\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "# === 1. One attention head ====================================================\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size: int, head_dim: int, dropout: float,\n",
    "                 bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, head_dim, bias=bias)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q_in, k_in, v_in, mask=None):                                 # x: (B, S, D)\n",
    "        # q, k, v = self.q(x), self.k(x), self.v(x)         # (B,S,d_h) each\n",
    "        q = self.q_proj(q_in) # (B, Seq_q, d_h)\n",
    "        k = self.k_proj(k_in) # (B, Seq_k, d_h)\n",
    "        v = self.v_proj(v_in) # (B, Seq_v, d_h)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(~mask, float(\"-inf\"))\n",
    "            \n",
    "        attn = scores.softmax(dim=-1)\n",
    "        attn = self.drop(attn)            # (B,S,S)\n",
    "        context = attn @ v \n",
    "        return context                                 # (B,S,d_h)\n",
    "\n",
    "# === 2. Multi-head self-attention =============================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 qkv_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "        head_dim = hidden_size // num_heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(hidden_size, head_dim, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k=None, v=None, mask=None):          \n",
    "        \"\"\"\n",
    "        q : (B, Seq_q, D)\n",
    "        k : (B, Seq_k, D)   defaults to q if None\n",
    "        v : (B, Seq_k, D)   defaults to k if None\n",
    "        mask : (1, 1, Seq_q, Seq_k) or None\n",
    "        \"\"\"\n",
    "        k = q if k is None else k\n",
    "        v = q if v is None else v\n",
    "\n",
    "        # run every head, collect their (B, Seq_q, d_h) outputs\n",
    "        head_outputs = [\n",
    "            head(q, k, v, mask) for head in self.heads\n",
    "        ]                                 # list length H\n",
    "        concat = torch.cat(head_outputs, dim=-1)  # (B, Seq_q, D)\n",
    "\n",
    "        out = self.out_proj(concat)  # cache the projection\n",
    "        # print(\"      [MultiHeadAttention] q:\", q.shape)\n",
    "        # print(\"      [MultiHeadAttention] k:\", k.shape)\n",
    "        # print(\"      [MultiHeadAttention] v:\", v.shape)\n",
    "        # print(\"      [MultiHeadAttention] output:\", out.shape)\n",
    "\n",
    "        return self.drop(out)\n",
    "\n",
    "# === 3. Position-wise feed-forward (MLP) ======================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 intermediate_size: int = HIDDEN_DIM * 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            NewGELUActivation(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):                                 # (B,S,D) -> (B,S,D)\n",
    "        return self.net(x)\n",
    "\n",
    "# === 4. Transformer block =====================================================\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 mlp_ratio: int = 4,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * mlp_ratio, dropout)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        x = x + self.attn(self.ln1(x))                    # SA + residual\n",
    "        x = x + self.mlp(self.ln2(x))                     # MLP + residual\n",
    "        return x                                          # (B,S,D)\n",
    "\n",
    "# === 5. Encoder = N stacked blocks ============================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, depth: int = NUM_ENCODER,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(hidden_size, num_heads, dropout=dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):                                 # (B,S,D)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.ln_final(x)                           # final norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "#  Synthetic 2×2–grid dataset (Stage-1 of our encoder-decoder project)\n",
    "#  Canvas: 56×56   PatchSize: 4  → 14×14 = 196 patch tokens\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "VOCAB = {str(i): i for i in range(10)}\n",
    "VOCAB['<start>']  = 10\n",
    "VOCAB['<finish>'] = 11\n",
    "VOCAB['<pad>']    = 12\n",
    "PAD_IDX      = VOCAB['<pad>']\n",
    "MAX_DECODER_LEN  = 128                    #  some big number\n",
    "VOCAB_INV = {v: k for k, v in VOCAB.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#causal mask \n",
    "def causal_mask(seq_len: int, device=None):\n",
    "    idx = torch.arange(seq_len, device=device)\n",
    "    mask = idx[:, None] >= idx[None, :]          # (T, T) bool\n",
    "    return mask[None, :, :]                      # (1, T, T)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.self_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = MLP(hidden_size, hidden_size * 4, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \"\"\"\n",
    "        x       : (B, T, D)   decoder input so far\n",
    "        enc_out : (B, S, D)   encoder memory\n",
    "        mask    : (1, 1, T, T) causal mask\n",
    "        \"\"\"\n",
    "        # 1. masked self-attention\n",
    "        qkv_in = self.ln1(x)\n",
    "        x = x + self.self_attn(qkv_in, qkv_in, qkv_in, mask)\n",
    "\n",
    "        # 2. encoder–decoder cross-attention\n",
    "        q = self.ln2(x)\n",
    "        k = v = enc_out                        # same tensor for key and value\n",
    "        x = x + self.cross_attn(q, k, v)       # no mask here\n",
    "\n",
    "        # 3. feed-forward\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DigitDecoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int = EMBED_DIM,\n",
    "                 num_heads: int = NUM_HEADS,\n",
    "                 dropout: float = DROPOUT,\n",
    "                 depth: int = 3, #number of decoder blocks stacked sequentially\n",
    "                 vocab_size: int = len(VOCAB)\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, hidden_size) \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, MAX_DECODER_LEN, hidden_size))\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(hidden_size, num_heads, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, targets, enc_out):\n",
    "        \"\"\"\n",
    "        targets : (B, T) integer ids including <start> at index 0\n",
    "        enc_out : (B, S, D)\n",
    "        returns\n",
    "        logits  : (B, T-1, vocab) – predictions for d1, d2, …, <finish>\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = targets.size()\n",
    "        \n",
    "        \n",
    "        token_inputs = targets[:, :-1] #(B, T-1)\n",
    "        # print(\"  token_inputs:\", token_inputs.shape)\n",
    "        x = self.token_embed(token_inputs) #(B, T-1, D)\n",
    "        # print(\"  embedded x:\", x.shape)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :] \n",
    "        # print(\"    [DecoderBlock] x input :\", x.shape)\n",
    "        \n",
    "        mask = causal_mask(T-1, x.device)        # (1,1,T-1,T-1)\n",
    "\n",
    "\n",
    "        #run through blocks \n",
    "        for blk in self.blocks: \n",
    "            x = blk(x, enc_out, mask) \n",
    "        \n",
    "        x = self.ln_final(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        # print(\"  logits before return:\", logits.shape)\n",
    "        return logits \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTranscriber(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        \n",
    "        #reuse PatchEmbedding and Encoder \n",
    "        self.embed = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "        self.encoder = Encoder(NUM_ENCODER, EMBED_DIM, NUM_HEADS, DROPOUT)\n",
    "        self.decoder = DigitDecoder(EMBED_DIM, NUM_HEADS, depth = 3)\n",
    "        \n",
    "    def forward(self, images, targets): \n",
    "        # enc_seq = self.encoder(self.embed(images))\n",
    "        # logits = self.decoder(targets, enc_seq)\n",
    "        # return logits \n",
    "        # print(\"INPUT images shape:\", images.shape)\n",
    "        x = self.embed(images)\n",
    "        # print(\"AFTER PatchEmbedding:\", x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(\"AFTER Encoder:\", x.shape)\n",
    "        logits = self.decoder(targets, x)\n",
    "        # print(\"AFTER Decoder:\", logits.shape)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "dummy_img = torch.randn(B, 1, 56, 56)          # batch of fake canvases\n",
    "dummy_tgt = torch.randint(0, 10, (B, 5))       # random digits 0-9\n",
    "model = GridTranscriber()\n",
    "out = model(dummy_img, dummy_tgt)\n",
    "# print(\"logits shape :\", out.shape)             # should be (2, 6, 13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download MNIST dataset \n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "grid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # uint8 → 0-1\n",
    "    transforms.Normalize([0.5], [0.5])      # centre to −1..1\n",
    "])\n",
    "\n",
    "class GridMNIST(Dataset):\n",
    "    def __init__(self, base_images, base_labels,\n",
    "                 epoch_size=60_000, rng=None):\n",
    "        self.base_images = base_images          # (N,28,28) uint8\n",
    "        self.base_labels = base_labels          # (N,)\n",
    "        self.epoch_size  = epoch_size\n",
    "        self.rng = np.random.default_rng(rng)\n",
    "\n",
    "        # indices per digit for fast balanced sampling\n",
    "        self.per_digit = {d: np.where(base_labels == d)[0] for d in range(10)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size\n",
    "\n",
    "    def _random_cells(self):\n",
    "        n = self.rng.integers(1, 17)\n",
    "        return self.rng.choice(16, size=n, replace=False)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cell_ids = self._random_cells()\n",
    "\n",
    "        canvas = np.zeros((CANVAS_PIX, CANVAS_PIX), dtype=np.uint8)\n",
    "        digits = []\n",
    "\n",
    "        for cell in cell_ids:\n",
    "            d = int(self.rng.integers(0, 10))\n",
    "            img_idx = self.rng.choice(self.per_digit[d])\n",
    "            digit_img = self.base_images[img_idx]\n",
    "            \n",
    "            # ↓↓ new: shrink 28×28 → 14×14 so 2×2 fits in 28×28\n",
    "            digit_img = TF.resize(Image.fromarray(digit_img), 14)\n",
    "            digit_img = np.array(digit_img, dtype=np.uint8)\n",
    "\n",
    "            row, col = divmod(cell, GRID_SIZE)\n",
    "            top, left = row*CELL_PIX, col*CELL_PIX\n",
    "            canvas[top:top+CELL_PIX, left:left+CELL_PIX] = digit_img\n",
    "\n",
    "            digits.append(d)\n",
    "\n",
    "        #build target sequence \n",
    "        odds  = sorted([d for d in digits if d % 2 == 1])\n",
    "        evens = sorted([d for d in digits if d % 2 == 0], reverse=True)\n",
    "        seq   = [VOCAB['<start>']] + odds + evens + [VOCAB['<finish>']]\n",
    "\n",
    "        return {\n",
    "            'image'  : grid_transforms(canvas),          # (1,56,56) float\n",
    "            'target' : torch.tensor(seq, dtype=torch.long),  # (6,)\n",
    "        }\n",
    "    \n",
    "# Collate function to add padding to the targets \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    images  = torch.stack([item['image'] for item in batch])  # shape: (B, 1, 56, 56)\n",
    "    targets = [item['target'] for item in batch]              # list of length-B tensors\n",
    "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=PAD_IDX)  # (B, T_max)\n",
    "    return {'image': images, 'target': padded_targets}\n",
    "\n",
    "\n",
    "\n",
    "# ─── build train / val / test loaders ───────────────────────────────────────\n",
    "train_grid = GridMNIST(train_dataset.data.numpy(),\n",
    "                       train_dataset.targets.numpy(),\n",
    "                       epoch_size=60_000, rng=RANDOM_SEED)\n",
    "\n",
    "val_grid   = GridMNIST(test_dataset.data.numpy(),   # we reuse MNIST test set\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+1)\n",
    "\n",
    "test_grid  = GridMNIST(test_dataset.data.numpy(),\n",
    "                       test_dataset.targets.numpy(),\n",
    "                       epoch_size=10_000, rng=RANDOM_SEED+2)\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# Decide how many CPU cores to devote to data loading.\n",
    "# Four to eight usually keeps the GPU fed without wasting resources.\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "NUM_WORKERS = min(8, os.cpu_count())        # 4-8 is a good starting range\n",
    "\n",
    "loader_kwargs = dict(\n",
    "    batch_size      = BATCH_SIZE,\n",
    "    num_workers     = NUM_WORKERS,          # <- key addition\n",
    "    pin_memory      = True,                 # speeds up host-to-device copy\n",
    "    persistent_workers = True,              # keeps workers alive across epochs\n",
    "    prefetch_factor = 4                     # each worker holds 4 batches ready\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_grid,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,       \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,           \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_grid,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,      \n",
    "    **loader_kwargs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "# quick sanity-check: show a random training sample\n",
    "sample = next(iter(train_dataloader))\n",
    "img_grid = sample['image'][0].squeeze().cpu()   # (56,56)\n",
    "target   = sample['target'][0].tolist()\n",
    "seq_txt = ' '.join([VOCAB_INV[t] for t in target   # after padding\n",
    "                    if t not in (PAD_IDX,)])\n",
    "\n",
    "plt.imshow(img_grid, cmap='gray')\n",
    "plt.title(f\"target sequence: {seq_txt}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up space in GPU memory \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Target Token Distribution (Training Set) ---\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 0.  Optimizer & Loss\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "criterion = nn.CrossEntropyLoss()          # PAD will be masked manually\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=ADAM_WEIGHT_DECAY,\n",
    "    betas=ADAM_BETAS\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  Exact‑match metric (ignores PAD)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def sequence_exact_match(pred_ids, gold_ids):\n",
    "    ok = (pred_ids == gold_ids) | (gold_ids == PAD_IDX)\n",
    "    return ok.all(dim=1).float().mean().item()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NEW: Print target token distribution\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n--- Target Token Distribution (Training Set) ---\")\n",
    "all_tokens = torch.cat([sample[\"target\"] for sample in train_grid]) # Assumes train_grid is loaded\n",
    "counts = torch.bincount(all_tokens, minlength=len(VOCAB)) # Assumes VOCAB is defined\n",
    "for i, c in enumerate(counts):\n",
    "    print(f\"{VOCAB_INV[i]:>8}: {c}\") # Assumes VOCAB_INV is defined\n",
    "print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Train / Validation loop\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "for epoch in range(EPOCHS):\n",
    "    # -------- TRAIN ------------------------------------------------------\n",
    "    model.train()\n",
    "    tr_loss = tr_em = tr_tok = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"train {epoch+1}/{EPOCHS}\"):\n",
    "        imgs    = batch[\"image\"].to(device)     # (B, 1, H, W)\n",
    "        targets = batch[\"target\"].to(device)    # (B, T)  <start>…<finish>\n",
    "\n",
    "        dec_in   = targets                      # (B, T)\n",
    "        gold_out = targets[:, 1:]               # (B, T‑1)\n",
    "\n",
    "        logits   = model(imgs, dec_in)          # (B, T‑1, V)\n",
    "        assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "        mask            = gold_out != PAD_IDX                     # (B, T‑1)\n",
    "        logits_flat     = logits.reshape(-1, len(VOCAB))          # (B*T, V)\n",
    "        gold_flat       = gold_out.reshape(-1)                    # (B*T,)\n",
    "\n",
    "        logits_masked   = logits_flat[mask.reshape(-1)]           # (~P, V)\n",
    "        gold_masked     = gold_flat[mask.reshape(-1)]             # (~P,)\n",
    "\n",
    "        loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds    = logits.argmax(dim=-1)\n",
    "\n",
    "        # ─── Token Accuracy (excluding PAD) ──────────────────────────────\n",
    "        tok_correct = (preds == gold_out) & mask\n",
    "        tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_em   += sequence_exact_match(preds, gold_out)\n",
    "        tr_tok  += tok_acc\n",
    "\n",
    "    tr_loss /= len(train_dataloader)\n",
    "    tr_em   /= len(train_dataloader)\n",
    "    tr_tok  /= len(train_dataloader)\n",
    "\n",
    "    # -------- VALIDATION -------------------------------------------------\n",
    "    model.eval()\n",
    "    val_loss = val_em = val_tok = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            imgs    = batch[\"image\"].to(device)\n",
    "            targets = batch[\"target\"].to(device)\n",
    "\n",
    "            dec_in   = targets\n",
    "            gold_out = targets[:, 1:]\n",
    "\n",
    "            logits   = model(imgs, dec_in)\n",
    "            assert logits.shape[:2] == gold_out.shape\n",
    "\n",
    "            mask            = gold_out != PAD_IDX\n",
    "            logits_flat     = logits.reshape(-1, len(VOCAB))\n",
    "            gold_flat       = gold_out.reshape(-1)\n",
    "\n",
    "            logits_masked   = logits_flat[mask.reshape(-1)]\n",
    "            gold_masked     = gold_flat[mask.reshape(-1)]\n",
    "\n",
    "            loss = criterion(logits_masked, gold_masked)\n",
    "\n",
    "            preds    = logits.argmax(dim=-1)\n",
    "\n",
    "            # ─── Token Accuracy (excluding PAD) ──────────────────────────\n",
    "            tok_correct = (preds == gold_out) & mask\n",
    "            tok_acc = tok_correct.sum().item() / mask.sum().item()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_em   += sequence_exact_match(preds, gold_out)\n",
    "            val_tok  += tok_acc\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_em   /= len(val_dataloader)\n",
    "    val_tok  /= len(val_dataloader)\n",
    "\n",
    "    # --- Sanity check: Visualize predictions vs gold for first epoch --- \n",
    "    if epoch == 0: \n",
    "        # Get the predictions and gold sequences from the *last* validation batch\n",
    "        # Note: .tolist() moves data from GPU to CPU if needed\n",
    "        idx = 0  # Index of the sample within the batch to visualize\n",
    "        if preds.shape[0] > idx: # Check if the batch has at least one sample\n",
    "            pred_seq = preds[idx].tolist()\n",
    "            gold_seq = gold_out[idx].tolist()\n",
    "            # Convert token IDs to strings, filtering out padding\n",
    "            pred_str = [VOCAB_INV[t] for t in pred_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            gold_str = [VOCAB_INV[t] for t in gold_seq if t != PAD_IDX] # Assumes VOCAB_INV and PAD_IDX defined\n",
    "            print(f\"-- Epoch {epoch+1} Validation Sample {idx} --\")\n",
    "            print(f\"PRED ({len(pred_str)} tokens): {' '.join(pred_str)}\")\n",
    "            print(f\"GOLD ({len(gold_str)} tokens): {' '.join(gold_str)}\")\n",
    "            print(\"-------------------------------------\")\n",
    "        else:\n",
    "            print(f\"-- Epoch {epoch+1} Validation: Last batch was empty or smaller than index {idx}, cannot show sample --\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} │ \"\n",
    "          f\"train loss {tr_loss:.4f} │ EM {tr_em:.4f} │ tok_acc {tr_tok:.4f} │ \"\n",
    "          f\"val loss {val_loss:.4f} │ EM {val_em:.4f} │ tok_acc {val_tok:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ──────────────────────────────────────────────────────────────────────────\n",
    "# Inference: greedy decoding on a single test image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0. Move model to the device you want to run on\n",
    "device = \"cpu\"        # ← change to \"cuda\" if you want GPU decoding\n",
    "model = model.to(device).eval()      # make sure this is your trained model\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Grab a single test image  (shape: 1×56×56)  and move to same device\n",
    "sample      = next(iter(test_dataloader))         # batch dict\n",
    "test_image  = sample[\"image\"][0].to(device)       # (1,56,56) tensor\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Minimal tokenizer wrapper around VOCAB / VOCAB_INV dicts\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab, vocab_inv):\n",
    "        self.token_to_idx = vocab\n",
    "        self.idx_to_token = vocab_inv\n",
    "\n",
    "tokenizer = SimpleTokenizer(VOCAB, VOCAB_INV)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Point to the patch‑embedding layer (lives in model.embed)\n",
    "patch_embed_layer = model.embed\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4.  Greedy‑decode (function you defined earlier)\n",
    "def preprocess(image, patch_embed):\n",
    "    image = image.unsqueeze(0)                # (1,1,H,W)\n",
    "    return patch_embed(image)                 # (1, seq_len, D)\n",
    "\n",
    "def decode_token_ids(token_ids, tokenizer):\n",
    "    return [tokenizer.idx_to_token[i] for i in token_ids]\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(model, test_image, patch_embed, tokenizer, max_len=10):\n",
    "    print(\"Starting greedy decode …\")\n",
    "    model.eval()\n",
    "\n",
    "    encoder_input  = preprocess(test_image, patch_embed)\n",
    "    encoder_output = model.encoder(encoder_input)\n",
    "\n",
    "    start_id   = tokenizer.token_to_idx[\"<start>\"]\n",
    "    pad_id     = tokenizer.token_to_idx[\"<pad>\"]\n",
    "    finish_id  = tokenizer.token_to_idx[\"<finish>\"]\n",
    "\n",
    "    decoder_input = torch.full((1, max_len), pad_id, dtype=torch.long).to(test_image.device)\n",
    "    decoder_input[0, 0] = start_id\n",
    "\n",
    "    for t in range(1, max_len):\n",
    "        if t == 1:\n",
    "            # DigitDecoder will slice off last token, so we need at least 2\n",
    "            input_tokens = decoder_input[:, :2]\n",
    "        else:\n",
    "            input_tokens = decoder_input[:, :t]\n",
    "\n",
    "        logits = model.decoder(input_tokens, encoder_output)   # (1,t-1,vocab)\n",
    "        if logits.shape[1] == 0:\n",
    "            print(\"Warning: decoder returned empty logits at step\", t)\n",
    "            break\n",
    "\n",
    "        next_id = logits[:, -1].argmax(-1)  # (1,)\n",
    "        decoder_input[0, t] = next_id.item()\n",
    "\n",
    "        print(f\"step {t}: {tokenizer.idx_to_token[next_id.item()]}\")\n",
    "        if next_id.item() == finish_id:\n",
    "            break\n",
    "\n",
    "    tokens = decode_token_ids(decoder_input[0].tolist(), tokenizer)\n",
    "    print(\"final sequence:\", tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5. Run it\n",
    "generated = greedy_decode(\n",
    "    model       = model,\n",
    "    test_image  = test_image,\n",
    "    patch_embed = patch_embed_layer,\n",
    "    tokenizer   = tokenizer,\n",
    "    max_len     = 8        # adjust to your task’s longest sequence\n",
    ")\n",
    "\n",
    "print(\"Generated tokens:\", generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
